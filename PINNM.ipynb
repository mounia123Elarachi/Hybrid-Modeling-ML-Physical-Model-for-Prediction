{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\maghr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = r'C:\\Users\\maghr\\OneDrive\\Documents\\Big Data\\projets\\PINN_notebook\\Full_dataset.xlsx'\n",
    "data = pd.read_excel(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output columns\n",
    "input_columns = ['Net Volume', 'Pulp Area', 'Froth surface area', 'Froth thickness', 'Air Flow rate', 'R_inf Ccp', 'R_inf Gn',\n",
    "                  'R_inf Po', 'R_inf Sp', 'k_max Ccp', 'k_max Gn', 'k_maxPo', 'k_max Sp', 'Entrainement Savassi parameters',\n",
    "                    'Total Solids Flow_Feed','Total Liquid Flow_Feed', 'Pulp Volumetric Flow_Feed', 'Solids SG_Feed',\n",
    "                      'Pulp SG_Feed', 'Solids Fraction_Feed', 'Cu_Feed', 'Fe_Feed', 'Pb_Feed', 'Zn_Feed']\n",
    "# input_columns = ['Total Solids Flow_Feed', 'Cu_Feed', 'Fe_Feed', 'Pb_Feed', 'Zn_/Feed']\n",
    "\n",
    "\n",
    "output_columns = ['Total Solids Flow_Concentrate', \n",
    "                   'Total Solids Flow_Tailings','Cu_Tails', 'Fe_Tails', 'Pb_Tails', 'Zn_Tails', \n",
    "                  'Cu_Concentrate', 'Fe_Concentrate', 'Pb_Concentrate', 'Zn_Concentrate']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X = data[input_columns]\n",
    "y = data[output_columns]\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=(2/9), random_state=42)\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the input data (X)\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_val_scaled = scaler_X.transform(X_val)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "# Fit and transform the output data (y)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "y_val_scaled = scaler_y.transform(y_val)\n",
    "y_test_scaled = scaler_y.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "input_shape = X_train.shape[1]\n",
    "output_shape = y_train.shape[1]\n",
    "lambda_physics = 0.9\n",
    "lambda_prediction = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_loss(y_true, y_pred):\n",
    "    \n",
    "# Extracting indices from the output_columns list\n",
    "    cu_concentrate_index = output_columns.index('Cu_Concentrate')\n",
    "    # print(cu_concentrate_index)\n",
    "    cu_tails_index = output_columns.index('Cu_Tails')\n",
    "    fe_concentrate_index = output_columns.index('Fe_Concentrate')\n",
    "    fe_tails_index = output_columns.index('Fe_Tails')\n",
    "    pb_concentrate_index = output_columns.index('Pb_Concentrate')\n",
    "    pb_tails_index = output_columns.index('Pb_Tails')\n",
    "    zn_concentrate_index = output_columns.index('Zn_Concentrate')\n",
    "    zn_tails_index = output_columns.index('Zn_Tails')\n",
    "    total_solid_flow_concentrate_index = output_columns.index('Total Solids Flow_Concentrate')\n",
    "    total_solid_flow_tailings_index = output_columns.index('Total Solids Flow_Tailings')\n",
    "\n",
    "\n",
    "    # Extracting predicted and actual values using the indices\n",
    "    Cu_concentrate_pred = y_pred[:, cu_concentrate_index]\n",
    "    Cu_tailing_pred = y_pred[:, cu_tails_index]\n",
    "    Fe_concentrate_pred = y_pred[:, fe_concentrate_index]\n",
    "    Fe_tailing_pred = y_pred[:, fe_tails_index]\n",
    "    Pb_concentrate_pred = y_pred[:, pb_concentrate_index]\n",
    "    Pb_tailing_pred = y_pred[:, pb_tails_index]\n",
    "    Zn_concentrate_pred = y_pred[:, zn_concentrate_index]\n",
    "    Zn_tailing_pred = y_pred[:, zn_tails_index]\n",
    "    TotalSolidFlow_concentrate_pred = y_pred[:, total_solid_flow_concentrate_index]\n",
    "    TotalSolidFlow_tailing_pred = y_pred[:, total_solid_flow_tailings_index]\n",
    "\n",
    "    Cu_concentrate_true = y_true[:, cu_concentrate_index]\n",
    "    Cu_tailing_true = y_true[:, cu_tails_index]\n",
    "    # tf.print(\"y_trueCUconce:\", Cu_concentrate_true )\n",
    "    Fe_concentrate_true = y_true[:, fe_concentrate_index]\n",
    "    Fe_tailing_true = y_true[:, fe_tails_index]\n",
    "    Pb_concentrate_true = y_true[:, pb_concentrate_index]\n",
    "    Pb_tailing_true = y_true[:, pb_tails_index]\n",
    "    Zn_concentrate_true = y_true[:, zn_concentrate_index]\n",
    "    Zn_tailing_true = y_true[:, zn_tails_index]\n",
    "    TotalSolidFlow_concentrate_true = y_true[:, total_solid_flow_concentrate_index]\n",
    "    TotalSolidFlow_tailing_true = y_true[:, total_solid_flow_tailings_index]\n",
    "    # tf.print(\"y_pred:\", y_pred)\n",
    "    # tf.print(\"y_true:\", y_true )\n",
    "    def mineral_loss(concentrate_pred, concentrate_true, tailing_pred, tailing_true, total_solid_concentrate_pred, total_solid_concentrate_true, total_solid_tailing_pred, total_solid_tailing_true):\n",
    "        loss_concentrate = tf.square((concentrate_pred * total_solid_concentrate_pred / 100) - (concentrate_true * total_solid_concentrate_true / 100))\n",
    "        loss_tailing = tf.square((tailing_pred * total_solid_tailing_pred / 100) - (tailing_true * total_solid_tailing_true / 100))\n",
    "        loss_total_solid_concentrate = tf.square(total_solid_concentrate_pred - total_solid_concentrate_true)\n",
    "        loss_total_solid_tailing = tf.square(total_solid_tailing_pred - total_solid_tailing_true)\n",
    "\n",
    "        return loss_concentrate + loss_tailing + loss_total_solid_concentrate + loss_total_solid_tailing\n",
    "    \n",
    "\n",
    "    # Calculate the loss for each mineral\n",
    "    loss_Cu = mineral_loss(Cu_concentrate_pred, Cu_concentrate_true, Cu_tailing_pred, Cu_tailing_true, TotalSolidFlow_concentrate_pred, TotalSolidFlow_concentrate_true, TotalSolidFlow_tailing_pred, TotalSolidFlow_tailing_true)\n",
    "    loss_Fe = mineral_loss(Fe_concentrate_pred, Fe_concentrate_true, Fe_tailing_pred, Fe_tailing_true, TotalSolidFlow_concentrate_pred, TotalSolidFlow_concentrate_true, TotalSolidFlow_tailing_pred, TotalSolidFlow_tailing_true)\n",
    "    loss_Pb = mineral_loss(Pb_concentrate_pred, Pb_concentrate_true, Pb_tailing_pred, Pb_tailing_true, TotalSolidFlow_concentrate_pred, TotalSolidFlow_concentrate_true, TotalSolidFlow_tailing_pred, TotalSolidFlow_tailing_true)\n",
    "    loss_Zn = mineral_loss(Zn_concentrate_pred, Zn_concentrate_true, Zn_tailing_pred, Zn_tailing_true, TotalSolidFlow_concentrate_pred, TotalSolidFlow_concentrate_true, TotalSolidFlow_tailing_pred, TotalSolidFlow_tailing_true)\n",
    "    \n",
    "    # Sum the losses for all minerals\n",
    "    total_predict_loss = loss_Cu + loss_Zn + loss_Fe + loss_Pb\n",
    "    return total_predict_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def physics_loss(y_true):\n",
    "    # Extracting indices from the output_columns list for true values\n",
    "    total_solid_flow_feed_index = min(input_columns.index('Total Solids Flow_Feed'), y_true.shape[1] - 1)\n",
    "    total_solid_flow_tailings_index = output_columns.index('Total Solids Flow_Tailings')\n",
    "    total_solid_flow_concentrate_index = output_columns.index('Total Solids Flow_Concentrate')\n",
    "    cu_feed_index = min(input_columns.index('Cu_Feed'), y_true.shape[1] - 1)\n",
    "    cu_tails_index = output_columns.index('Cu_Tails')\n",
    "    cu_concentrate_index = output_columns.index('Cu_Concentrate')\n",
    "    fe_feed_index = min(input_columns.index('Fe_Feed'), y_true.shape[1] - 1)\n",
    "    fe_tails_index = output_columns.index('Fe_Tails')\n",
    "    fe_concentrate_index = output_columns.index('Fe_Concentrate')\n",
    "    pb_feed_index = min(input_columns.index('Pb_Feed'), y_true.shape[1] - 1)\n",
    "    pb_tails_index = output_columns.index('Pb_Tails')\n",
    "    pb_concentrate_index = output_columns.index('Pb_Concentrate')\n",
    "    zn_feed_index = min(input_columns.index('Zn_Feed'), y_true.shape[1] - 1)\n",
    "    zn_tails_index = output_columns.index('Zn_Tails')\n",
    "    zn_concentrate_index = output_columns.index('Zn_Concentrate')\n",
    "\n",
    "    # Extracting true values using the indices\n",
    "    TotalSolidFlow_feed_true = y_true[:, total_solid_flow_feed_index]\n",
    "    TotalSolidFlow_tailings_true = y_true[:, total_solid_flow_tailings_index]\n",
    "    TotalSolidFlow_concentrate_true = y_true[:, total_solid_flow_concentrate_index]\n",
    "    Cu_feed_true = y_true[:, cu_feed_index]\n",
    "    Cu_tailings_true = y_true[:, cu_tails_index]\n",
    "    Cu_concentrate_true = y_true[:, cu_concentrate_index]\n",
    "    Fe_feed_true = y_true[:, fe_feed_index]\n",
    "    Fe_tailings_true = y_true[:, fe_tails_index]\n",
    "    Fe_concentrate_true = y_true[:, fe_concentrate_index]\n",
    "    Pb_feed_true = y_true[:, pb_feed_index]\n",
    "    Pb_tailings_true = y_true[:, pb_tails_index]\n",
    "    Pb_concentrate_true = y_true[:, pb_concentrate_index]\n",
    "    Zn_feed_true = y_true[:, zn_feed_index]\n",
    "    Zn_tailings_true = y_true[:, zn_tails_index]\n",
    "    Zn_concentrate_true = y_true[:, zn_concentrate_index]\n",
    "    loss_Cu = tf.square((Cu_feed_true * TotalSolidFlow_feed_true / 100) - ((Cu_tailings_true * TotalSolidFlow_tailings_true / 100) + (Cu_concentrate_true * TotalSolidFlow_concentrate_true / 100)))\n",
    "    loss_Fe = tf.square((Fe_feed_true * TotalSolidFlow_feed_true / 100) - ((Fe_tailings_true * TotalSolidFlow_tailings_true / 100) + (Fe_concentrate_true * TotalSolidFlow_concentrate_true / 100)))\n",
    "    loss_Pb = tf.square((Pb_feed_true * TotalSolidFlow_feed_true / 100) - ((Pb_tailings_true * TotalSolidFlow_tailings_true / 100) + (Pb_concentrate_true * TotalSolidFlow_concentrate_true / 100)))\n",
    "    loss_Zn = tf.square((Zn_feed_true * TotalSolidFlow_feed_true / 100) - ((Zn_tailings_true * TotalSolidFlow_tailings_true / 100) + (Zn_concentrate_true * TotalSolidFlow_concentrate_true / 100)))\n",
    "    # print(loss_Cu)\n",
    "    # tf.print(\"y_true:\", y_true)\n",
    "\n",
    "    # Combine the physics-based losses for all minerals\n",
    "    total_physics_loss = loss_Cu + loss_Fe + loss_Pb + loss_Zn\n",
    "    return total_physics_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_loss_function(y_true, y_pred):\n",
    "    total_predict_loss = predict_loss(y_true, y_pred)\n",
    "    total_physics_loss = physics_loss(y_true)\n",
    "    total_loss = (lambda_physics * total_physics_loss) + (lambda_prediction * total_predict_loss)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(input_shape,))\n",
    "x = Dense(128, activation='relu')(input_layer)  # Input layer\n",
    "x = Dense(256, activation='relu')(x)           # 1st hidden layer\n",
    "x = Dense(512, activation='relu')(x)           # 2nd hidden layer\n",
    "x = Dense(1024, activation='relu')(x)          # 3rd hidden layer\n",
    "output_layer = Dense(len(output_columns), activation='linear')(x)  # Adjust this to match the number of output columns\n",
    "\n",
    "# input_layer = Input(shape=(input_shape,))\n",
    "# x = Dense(128, activation='tanh')(input_layer)  # Input layer\n",
    "# x = Dense(256, activation='tanh')(x)           # 1st hidden layer\n",
    "# x = Dense(512, activation='tanh')(x)           # 2nd hidden layer\n",
    "# x = Dense(1024, activation='tanh')(x)          # 3rd hidden layer\n",
    "# output_layer = Dense(output_shape, activation='linear')(x)  # Output layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "              loss=total_loss_function, metrics='Accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "84/84 [==============================] - 10s 39ms/step - loss: 4.4358 - Accuracy: 0.2201 - val_loss: 0.0308 - val_Accuracy: 0.2799\n",
      "Epoch 2/500\n",
      "84/84 [==============================] - 3s 34ms/step - loss: 0.0286 - Accuracy: 0.3042 - val_loss: 0.0142 - val_Accuracy: 0.3127\n",
      "Epoch 3/500\n",
      "84/84 [==============================] - 3s 32ms/step - loss: 0.0133 - Accuracy: 0.3305 - val_loss: 0.0224 - val_Accuracy: 0.3574\n",
      "Epoch 4/500\n",
      "84/84 [==============================] - 3s 32ms/step - loss: 0.0098 - Accuracy: 0.3372 - val_loss: 0.0071 - val_Accuracy: 0.3561\n",
      "Epoch 5/500\n",
      "84/84 [==============================] - 2s 29ms/step - loss: 0.0069 - Accuracy: 0.3522 - val_loss: 0.0061 - val_Accuracy: 0.3719\n",
      "Epoch 6/500\n",
      "84/84 [==============================] - 3s 30ms/step - loss: 0.0059 - Accuracy: 0.3597 - val_loss: 0.0049 - val_Accuracy: 0.3784\n",
      "Epoch 7/500\n",
      "84/84 [==============================] - 2s 29ms/step - loss: 0.0052 - Accuracy: 0.3579 - val_loss: 0.0070 - val_Accuracy: 0.3640\n",
      "Epoch 8/500\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.0038 - Accuracy: 0.3432 - val_loss: 0.0027 - val_Accuracy: 0.3601\n",
      "Epoch 9/500\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.0033 - Accuracy: 0.3511 - val_loss: 0.0028 - val_Accuracy: 0.3614\n",
      "Epoch 10/500\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.0044 - Accuracy: 0.3443 - val_loss: 0.0110 - val_Accuracy: 0.3732\n",
      "Epoch 11/500\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0048 - Accuracy: 0.3485 - val_loss: 0.0106 - val_Accuracy: 0.3745\n",
      "Epoch 12/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0036 - Accuracy: 0.3526 - val_loss: 0.0020 - val_Accuracy: 0.3443\n",
      "Epoch 13/500\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0033 - Accuracy: 0.3492 - val_loss: 0.0026 - val_Accuracy: 0.3837\n",
      "Epoch 14/500\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0026 - Accuracy: 0.3564 - val_loss: 0.0018 - val_Accuracy: 0.3614\n",
      "Epoch 15/500\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0040 - Accuracy: 0.3526 - val_loss: 0.0038 - val_Accuracy: 0.3693\n",
      "Epoch 16/500\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0038 - Accuracy: 0.3507 - val_loss: 0.0030 - val_Accuracy: 0.3837\n",
      "Epoch 17/500\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.0037 - Accuracy: 0.3432 - val_loss: 0.0034 - val_Accuracy: 0.3574\n",
      "Epoch 18/500\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.0030 - Accuracy: 0.3492 - val_loss: 0.0073 - val_Accuracy: 0.3522\n",
      "Epoch 19/500\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0024 - Accuracy: 0.3436 - val_loss: 0.0029 - val_Accuracy: 0.3561\n",
      "Epoch 20/500\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.0031 - Accuracy: 0.3417 - val_loss: 0.0045 - val_Accuracy: 0.3482\n",
      "Epoch 21/500\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.0033 - Accuracy: 0.3567 - val_loss: 0.0017 - val_Accuracy: 0.3679\n",
      "Epoch 22/500\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0035 - Accuracy: 0.3432 - val_loss: 0.0027 - val_Accuracy: 0.3469\n",
      "Epoch 23/500\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.0026 - Accuracy: 0.3579 - val_loss: 0.0028 - val_Accuracy: 0.3338\n",
      "Epoch 24/500\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0036 - Accuracy: 0.3507 - val_loss: 0.0028 - val_Accuracy: 0.3614\n",
      "Epoch 25/500\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0025 - Accuracy: 0.3545 - val_loss: 0.0019 - val_Accuracy: 0.3811\n",
      "Epoch 26/500\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0029 - Accuracy: 0.3571 - val_loss: 0.0027 - val_Accuracy: 0.3758\n",
      "Epoch 27/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0028 - Accuracy: 0.3564 - val_loss: 0.0040 - val_Accuracy: 0.3653\n",
      "Epoch 28/500\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0024 - Accuracy: 0.3714 - val_loss: 0.0022 - val_Accuracy: 0.3890\n",
      "Epoch 29/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0024 - Accuracy: 0.3680 - val_loss: 0.0053 - val_Accuracy: 0.3614\n",
      "Epoch 30/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0043 - Accuracy: 0.3575 - val_loss: 0.0020 - val_Accuracy: 0.4047\n",
      "Epoch 31/500\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0020 - Accuracy: 0.3823 - val_loss: 0.0015 - val_Accuracy: 0.3693\n",
      "Epoch 32/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0022 - Accuracy: 0.3755 - val_loss: 0.0014 - val_Accuracy: 0.3693\n",
      "Epoch 33/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0021 - Accuracy: 0.3808 - val_loss: 0.0016 - val_Accuracy: 0.3784\n",
      "Epoch 34/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0023 - Accuracy: 0.3823 - val_loss: 0.0037 - val_Accuracy: 0.4087\n",
      "Epoch 35/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0027 - Accuracy: 0.3913 - val_loss: 0.0026 - val_Accuracy: 0.4100\n",
      "Epoch 36/500\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0028 - Accuracy: 0.3857 - val_loss: 0.0060 - val_Accuracy: 0.4021\n",
      "Epoch 37/500\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0041 - Accuracy: 0.3819 - val_loss: 0.0030 - val_Accuracy: 0.3745\n",
      "Epoch 38/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0024 - Accuracy: 0.3789 - val_loss: 0.0021 - val_Accuracy: 0.3982\n",
      "Epoch 39/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0021 - Accuracy: 0.3887 - val_loss: 0.0026 - val_Accuracy: 0.3929\n",
      "Epoch 40/500\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0021 - Accuracy: 0.3920 - val_loss: 0.0042 - val_Accuracy: 0.4113\n",
      "Epoch 41/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0023 - Accuracy: 0.3868 - val_loss: 0.0031 - val_Accuracy: 0.4166\n",
      "Epoch 42/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0026 - Accuracy: 0.3830 - val_loss: 0.0021 - val_Accuracy: 0.4179\n",
      "Epoch 43/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0018 - Accuracy: 0.3962 - val_loss: 0.0056 - val_Accuracy: 0.3784\n",
      "Epoch 44/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0027 - Accuracy: 0.3890 - val_loss: 0.0026 - val_Accuracy: 0.3995\n",
      "Epoch 45/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0022 - Accuracy: 0.3992 - val_loss: 0.0054 - val_Accuracy: 0.4021\n",
      "Epoch 46/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0031 - Accuracy: 0.3872 - val_loss: 0.0032 - val_Accuracy: 0.4021\n",
      "Epoch 47/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0027 - Accuracy: 0.3879 - val_loss: 0.0020 - val_Accuracy: 0.4074\n",
      "Epoch 48/500\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0033 - Accuracy: 0.3849 - val_loss: 0.0029 - val_Accuracy: 0.4074\n",
      "Epoch 49/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0021 - Accuracy: 0.3917 - val_loss: 0.0050 - val_Accuracy: 0.3666\n",
      "Epoch 50/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0021 - Accuracy: 0.3980 - val_loss: 0.0021 - val_Accuracy: 0.3955\n",
      "Epoch 51/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0016 - Accuracy: 0.3999 - val_loss: 0.0017 - val_Accuracy: 0.3995\n",
      "Epoch 52/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0019 - Accuracy: 0.3890 - val_loss: 0.0028 - val_Accuracy: 0.4113\n",
      "Epoch 53/500\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0022 - Accuracy: 0.3969 - val_loss: 0.0021 - val_Accuracy: 0.3837\n",
      "Epoch 54/500\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.0019 - Accuracy: 0.3954 - val_loss: 0.0020 - val_Accuracy: 0.4126\n",
      "Epoch 55/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0020 - Accuracy: 0.3868 - val_loss: 0.0015 - val_Accuracy: 0.4074\n",
      "Epoch 56/500\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0018 - Accuracy: 0.3868 - val_loss: 0.0019 - val_Accuracy: 0.4034\n",
      "Epoch 57/500\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0019 - Accuracy: 0.3973 - val_loss: 0.0017 - val_Accuracy: 0.3903\n",
      "Epoch 58/500\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0018 - Accuracy: 0.3905 - val_loss: 0.0013 - val_Accuracy: 0.4034\n",
      "Epoch 59/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0019 - Accuracy: 0.3932 - val_loss: 0.0053 - val_Accuracy: 0.3469\n",
      "Epoch 60/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0027 - Accuracy: 0.3748 - val_loss: 0.0032 - val_Accuracy: 0.4179\n",
      "Epoch 61/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0019 - Accuracy: 0.3928 - val_loss: 0.0026 - val_Accuracy: 0.4126\n",
      "Epoch 62/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0025 - Accuracy: 0.3954 - val_loss: 0.0027 - val_Accuracy: 0.3850\n",
      "Epoch 63/500\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0029 - Accuracy: 0.3860 - val_loss: 0.0028 - val_Accuracy: 0.3942\n",
      "Epoch 64/500\n",
      "84/84 [==============================] - 3s 32ms/step - loss: 0.0022 - Accuracy: 0.3755 - val_loss: 0.0014 - val_Accuracy: 0.3942\n",
      "Epoch 65/500\n",
      "84/84 [==============================] - 3s 31ms/step - loss: 0.0018 - Accuracy: 0.3834 - val_loss: 0.0030 - val_Accuracy: 0.3850\n",
      "Epoch 66/500\n",
      "84/84 [==============================] - 2s 28ms/step - loss: 0.0017 - Accuracy: 0.3823 - val_loss: 0.0021 - val_Accuracy: 0.3995\n",
      "Epoch 67/500\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.0023 - Accuracy: 0.3898 - val_loss: 0.0021 - val_Accuracy: 0.4021\n",
      "Epoch 68/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0016 - Accuracy: 0.3706 - val_loss: 0.0027 - val_Accuracy: 0.4087\n",
      "Epoch 69/500\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 0.0021 - Accuracy: 0.3804 - val_loss: 0.0023 - val_Accuracy: 0.4218\n",
      "Epoch 70/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0019 - Accuracy: 0.3763 - val_loss: 0.0019 - val_Accuracy: 0.3955\n",
      "Epoch 71/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0015 - Accuracy: 0.3770 - val_loss: 0.0018 - val_Accuracy: 0.3798\n",
      "Epoch 72/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0017 - Accuracy: 0.3827 - val_loss: 0.0011 - val_Accuracy: 0.3968\n",
      "Epoch 73/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0017 - Accuracy: 0.3800 - val_loss: 0.0012 - val_Accuracy: 0.4047\n",
      "Epoch 74/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0014 - Accuracy: 0.3740 - val_loss: 0.0021 - val_Accuracy: 0.4231\n",
      "Epoch 75/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0016 - Accuracy: 0.3759 - val_loss: 0.0045 - val_Accuracy: 0.3745\n",
      "Epoch 76/500\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0019 - Accuracy: 0.3789 - val_loss: 0.0048 - val_Accuracy: 0.4034\n",
      "Epoch 77/500\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.0019 - Accuracy: 0.3763 - val_loss: 0.0021 - val_Accuracy: 0.3706\n",
      "Epoch 78/500\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0019 - Accuracy: 0.3785 - val_loss: 0.0016 - val_Accuracy: 0.3929\n",
      "Epoch 79/500\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0021 - Accuracy: 0.3823 - val_loss: 0.0023 - val_Accuracy: 0.3679\n",
      "Epoch 80/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0037 - Accuracy: 0.3594 - val_loss: 0.0025 - val_Accuracy: 0.3850\n",
      "Epoch 81/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0021 - Accuracy: 0.3688 - val_loss: 0.0017 - val_Accuracy: 0.3824\n",
      "Epoch 82/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0018 - Accuracy: 0.3751 - val_loss: 0.0015 - val_Accuracy: 0.4139\n",
      "Epoch 83/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0019 - Accuracy: 0.3703 - val_loss: 0.0021 - val_Accuracy: 0.3968\n",
      "Epoch 84/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0016 - Accuracy: 0.3830 - val_loss: 0.0014 - val_Accuracy: 0.3916\n",
      "Epoch 85/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0015 - Accuracy: 0.3883 - val_loss: 0.0016 - val_Accuracy: 0.3942\n",
      "Epoch 86/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0016 - Accuracy: 0.3740 - val_loss: 0.0013 - val_Accuracy: 0.4139\n",
      "Epoch 87/500\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0016 - Accuracy: 0.3800 - val_loss: 0.0011 - val_Accuracy: 0.4179\n",
      "Epoch 88/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0016 - Accuracy: 0.3917 - val_loss: 0.0013 - val_Accuracy: 0.4258\n",
      "Epoch 89/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0016 - Accuracy: 0.3853 - val_loss: 0.0027 - val_Accuracy: 0.3706\n",
      "Epoch 90/500\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0016 - Accuracy: 0.3958 - val_loss: 0.0018 - val_Accuracy: 0.4100\n",
      "Epoch 91/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0035 - Accuracy: 0.3879 - val_loss: 0.0028 - val_Accuracy: 0.3863\n",
      "Epoch 92/500\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0016 - Accuracy: 0.3939 - val_loss: 0.0030 - val_Accuracy: 0.4087\n",
      "Epoch 93/500\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0025 - Accuracy: 0.3965 - val_loss: 0.0055 - val_Accuracy: 0.4297\n",
      "Epoch 94/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0023 - Accuracy: 0.3932 - val_loss: 0.0030 - val_Accuracy: 0.4310\n",
      "Epoch 95/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0015 - Accuracy: 0.3999 - val_loss: 0.0019 - val_Accuracy: 0.4152\n",
      "Epoch 96/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0015 - Accuracy: 0.3995 - val_loss: 0.0018 - val_Accuracy: 0.4192\n",
      "Epoch 97/500\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0015 - Accuracy: 0.3928 - val_loss: 0.0020 - val_Accuracy: 0.4179\n",
      "Epoch 98/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0016 - Accuracy: 0.3980 - val_loss: 0.0015 - val_Accuracy: 0.4152\n",
      "Epoch 99/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0012 - Accuracy: 0.4082 - val_loss: 0.0019 - val_Accuracy: 0.3837\n",
      "Epoch 100/500\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0014 - Accuracy: 0.4056 - val_loss: 0.0016 - val_Accuracy: 0.4244\n",
      "Epoch 101/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0015 - Accuracy: 0.4026 - val_loss: 0.0027 - val_Accuracy: 0.4415\n",
      "Epoch 102/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0014 - Accuracy: 0.4067 - val_loss: 0.0018 - val_Accuracy: 0.4152\n",
      "Epoch 103/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0016 - Accuracy: 0.4011 - val_loss: 0.0013 - val_Accuracy: 0.4310\n",
      "Epoch 104/500\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0014 - Accuracy: 0.4052 - val_loss: 0.0016 - val_Accuracy: 0.4310\n",
      "Epoch 105/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0017 - Accuracy: 0.3969 - val_loss: 0.0017 - val_Accuracy: 0.4244\n",
      "Epoch 106/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0014 - Accuracy: 0.4089 - val_loss: 0.0015 - val_Accuracy: 0.3890\n",
      "Epoch 107/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0015 - Accuracy: 0.4108 - val_loss: 0.0041 - val_Accuracy: 0.4139\n",
      "Epoch 108/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0033 - Accuracy: 0.4014 - val_loss: 0.0014 - val_Accuracy: 0.4323\n",
      "Epoch 109/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0018 - Accuracy: 0.3984 - val_loss: 0.0025 - val_Accuracy: 0.4113\n",
      "Epoch 110/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0016 - Accuracy: 0.3958 - val_loss: 0.0019 - val_Accuracy: 0.4271\n",
      "Epoch 111/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0022 - Accuracy: 0.3962 - val_loss: 0.0013 - val_Accuracy: 0.4258\n",
      "Epoch 112/500\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0018 - Accuracy: 0.4011 - val_loss: 0.0018 - val_Accuracy: 0.3942\n",
      "Epoch 113/500\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0015 - Accuracy: 0.4037 - val_loss: 0.0013 - val_Accuracy: 0.4297\n",
      "Epoch 114/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0016 - Accuracy: 0.4014 - val_loss: 0.0037 - val_Accuracy: 0.4244\n",
      "Epoch 115/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0016 - Accuracy: 0.3913 - val_loss: 0.0015 - val_Accuracy: 0.4271\n",
      "Epoch 116/500\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0021 - Accuracy: 0.3939 - val_loss: 0.0014 - val_Accuracy: 0.3982\n",
      "Epoch 117/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0013 - Accuracy: 0.3958 - val_loss: 0.0013 - val_Accuracy: 0.3863\n",
      "Epoch 118/500\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 0.0014 - Accuracy: 0.3973 - val_loss: 0.0017 - val_Accuracy: 0.4415\n",
      "Epoch 119/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0016 - Accuracy: 0.3999 - val_loss: 0.0016 - val_Accuracy: 0.3824\n",
      "Epoch 120/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0015 - Accuracy: 0.3905 - val_loss: 0.0022 - val_Accuracy: 0.3968\n",
      "Epoch 121/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0018 - Accuracy: 0.3913 - val_loss: 0.0019 - val_Accuracy: 0.4126\n",
      "Epoch 122/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0016 - Accuracy: 0.3909 - val_loss: 0.0012 - val_Accuracy: 0.4218\n",
      "Epoch 123/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0016 - Accuracy: 0.3913 - val_loss: 0.0016 - val_Accuracy: 0.4231\n",
      "Epoch 124/500\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 0.0017 - Accuracy: 0.3917 - val_loss: 0.0018 - val_Accuracy: 0.4218\n",
      "Epoch 125/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0015 - Accuracy: 0.3857 - val_loss: 0.0013 - val_Accuracy: 0.4113\n",
      "Epoch 126/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0018 - Accuracy: 0.3917 - val_loss: 0.0013 - val_Accuracy: 0.4179\n",
      "Epoch 127/500\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 0.0013 - Accuracy: 0.3804 - val_loss: 0.0032 - val_Accuracy: 0.4310\n",
      "Epoch 128/500\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0015 - Accuracy: 0.3883 - val_loss: 0.0017 - val_Accuracy: 0.3955\n",
      "Epoch 129/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0014 - Accuracy: 0.3808 - val_loss: 0.0017 - val_Accuracy: 0.4271\n",
      "Epoch 130/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0014 - Accuracy: 0.3845 - val_loss: 0.0017 - val_Accuracy: 0.4060\n",
      "Epoch 131/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0013 - Accuracy: 0.3815 - val_loss: 0.0016 - val_Accuracy: 0.3666\n",
      "Epoch 132/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0015 - Accuracy: 0.3751 - val_loss: 0.0012 - val_Accuracy: 0.3719\n",
      "Epoch 133/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0014 - Accuracy: 0.3691 - val_loss: 0.0034 - val_Accuracy: 0.4060\n",
      "Epoch 134/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0018 - Accuracy: 0.3688 - val_loss: 0.0021 - val_Accuracy: 0.3495\n",
      "Epoch 135/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0017 - Accuracy: 0.3733 - val_loss: 0.0016 - val_Accuracy: 0.3561\n",
      "Epoch 136/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0013 - Accuracy: 0.3838 - val_loss: 0.0013 - val_Accuracy: 0.3771\n",
      "Epoch 137/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0016 - Accuracy: 0.3654 - val_loss: 0.0038 - val_Accuracy: 0.3679\n",
      "Epoch 138/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0019 - Accuracy: 0.3639 - val_loss: 0.0021 - val_Accuracy: 0.3417\n",
      "Epoch 139/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0013 - Accuracy: 0.3601 - val_loss: 0.0012 - val_Accuracy: 0.3666\n",
      "Epoch 140/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0017 - Accuracy: 0.3661 - val_loss: 0.0019 - val_Accuracy: 0.3535\n",
      "Epoch 141/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0018 - Accuracy: 0.3575 - val_loss: 0.0024 - val_Accuracy: 0.3574\n",
      "Epoch 142/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0016 - Accuracy: 0.3597 - val_loss: 0.0013 - val_Accuracy: 0.3640\n",
      "Epoch 143/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0015 - Accuracy: 0.3601 - val_loss: 0.0013 - val_Accuracy: 0.3640\n",
      "Epoch 144/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0014 - Accuracy: 0.3601 - val_loss: 0.0017 - val_Accuracy: 0.3443\n",
      "Epoch 145/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0017 - Accuracy: 0.3586 - val_loss: 0.0015 - val_Accuracy: 0.3601\n",
      "Epoch 146/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0015 - Accuracy: 0.3601 - val_loss: 0.0015 - val_Accuracy: 0.3364\n",
      "Epoch 147/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0013 - Accuracy: 0.3688 - val_loss: 0.0019 - val_Accuracy: 0.3784\n",
      "Epoch 148/500\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 0.0014 - Accuracy: 0.3575 - val_loss: 0.0014 - val_Accuracy: 0.3784\n",
      "Epoch 149/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0014 - Accuracy: 0.3646 - val_loss: 0.0030 - val_Accuracy: 0.3469\n",
      "Epoch 150/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0017 - Accuracy: 0.3526 - val_loss: 0.0018 - val_Accuracy: 0.3745\n",
      "Epoch 151/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0017 - Accuracy: 0.3515 - val_loss: 0.0027 - val_Accuracy: 0.3719\n",
      "Epoch 152/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0024 - Accuracy: 0.3601 - val_loss: 0.0028 - val_Accuracy: 0.3601\n",
      "Epoch 153/500\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 0.0019 - Accuracy: 0.3556 - val_loss: 0.0018 - val_Accuracy: 0.3561\n",
      "Epoch 154/500\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 0.0020 - Accuracy: 0.3612 - val_loss: 0.0030 - val_Accuracy: 0.3311\n",
      "Epoch 155/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0021 - Accuracy: 0.3639 - val_loss: 0.0016 - val_Accuracy: 0.3587\n",
      "Epoch 156/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0015 - Accuracy: 0.3669 - val_loss: 0.0014 - val_Accuracy: 0.3719\n",
      "Epoch 157/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0013 - Accuracy: 0.3714 - val_loss: 0.0016 - val_Accuracy: 0.3679\n",
      "Epoch 158/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0013 - Accuracy: 0.3669 - val_loss: 9.8494e-04 - val_Accuracy: 0.3587\n",
      "Epoch 159/500\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 0.0014 - Accuracy: 0.3770 - val_loss: 0.0012 - val_Accuracy: 0.3745\n",
      "Epoch 160/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0013 - Accuracy: 0.3744 - val_loss: 0.0017 - val_Accuracy: 0.3601\n",
      "Epoch 161/500\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 0.0013 - Accuracy: 0.3706 - val_loss: 0.0013 - val_Accuracy: 0.3995\n",
      "Epoch 162/500\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 0.0014 - Accuracy: 0.3796 - val_loss: 0.0021 - val_Accuracy: 0.3587\n",
      "Epoch 163/500\n",
      "84/84 [==============================] - 2s 27ms/step - loss: 0.0015 - Accuracy: 0.3620 - val_loss: 0.0016 - val_Accuracy: 0.3522\n",
      "Epoch 164/500\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.0018 - Accuracy: 0.3676 - val_loss: 0.0014 - val_Accuracy: 0.3679\n",
      "Epoch 165/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0013 - Accuracy: 0.3658 - val_loss: 0.0016 - val_Accuracy: 0.3403\n",
      "Epoch 166/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0017 - Accuracy: 0.3684 - val_loss: 0.0018 - val_Accuracy: 0.3509\n",
      "Epoch 167/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0016 - Accuracy: 0.3680 - val_loss: 0.0031 - val_Accuracy: 0.3535\n",
      "Epoch 168/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0020 - Accuracy: 0.3609 - val_loss: 0.0016 - val_Accuracy: 0.3403\n",
      "Epoch 169/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0015 - Accuracy: 0.3586 - val_loss: 0.0012 - val_Accuracy: 0.3509\n",
      "Epoch 170/500\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 0.0015 - Accuracy: 0.3624 - val_loss: 0.0016 - val_Accuracy: 0.3811\n",
      "Epoch 171/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0015 - Accuracy: 0.3594 - val_loss: 0.0012 - val_Accuracy: 0.3719\n",
      "Epoch 172/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0014 - Accuracy: 0.3710 - val_loss: 0.0013 - val_Accuracy: 0.4008\n",
      "Epoch 173/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0017 - Accuracy: 0.3695 - val_loss: 0.0013 - val_Accuracy: 0.3745\n",
      "Epoch 174/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0014 - Accuracy: 0.3601 - val_loss: 0.0023 - val_Accuracy: 0.3535\n",
      "Epoch 175/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0019 - Accuracy: 0.3684 - val_loss: 0.0021 - val_Accuracy: 0.3824\n",
      "Epoch 176/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0017 - Accuracy: 0.3635 - val_loss: 0.0013 - val_Accuracy: 0.3732\n",
      "Epoch 177/500\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 0.0014 - Accuracy: 0.3729 - val_loss: 0.0014 - val_Accuracy: 0.4074\n",
      "Epoch 178/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0014 - Accuracy: 0.3669 - val_loss: 0.0016 - val_Accuracy: 0.3798\n",
      "Epoch 179/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0017 - Accuracy: 0.3778 - val_loss: 0.0011 - val_Accuracy: 0.3732\n",
      "Epoch 180/500\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 0.0015 - Accuracy: 0.3680 - val_loss: 0.0015 - val_Accuracy: 0.3640\n",
      "Epoch 181/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0015 - Accuracy: 0.3699 - val_loss: 0.0013 - val_Accuracy: 0.3903\n",
      "Epoch 182/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0015 - Accuracy: 0.3658 - val_loss: 0.0014 - val_Accuracy: 0.3942\n",
      "Epoch 183/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0015 - Accuracy: 0.3778 - val_loss: 0.0014 - val_Accuracy: 0.3745\n",
      "Epoch 184/500\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 0.0014 - Accuracy: 0.3699 - val_loss: 0.0011 - val_Accuracy: 0.3574\n",
      "Epoch 185/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0013 - Accuracy: 0.3755 - val_loss: 0.0014 - val_Accuracy: 0.3706\n",
      "Epoch 186/500\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0014 - Accuracy: 0.3695 - val_loss: 0.0011 - val_Accuracy: 0.3285\n",
      "Epoch 187/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0014 - Accuracy: 0.3796 - val_loss: 0.0012 - val_Accuracy: 0.3745\n",
      "Epoch 188/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0014 - Accuracy: 0.3699 - val_loss: 0.0016 - val_Accuracy: 0.4021\n",
      "Epoch 189/500\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 0.0014 - Accuracy: 0.3808 - val_loss: 0.0010 - val_Accuracy: 0.3640\n",
      "Epoch 190/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0015 - Accuracy: 0.3778 - val_loss: 0.0013 - val_Accuracy: 0.3679\n",
      "Epoch 191/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0012 - Accuracy: 0.3785 - val_loss: 0.0014 - val_Accuracy: 0.3601\n",
      "Epoch 192/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0015 - Accuracy: 0.3661 - val_loss: 0.0021 - val_Accuracy: 0.3863\n",
      "Epoch 193/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0015 - Accuracy: 0.3676 - val_loss: 0.0031 - val_Accuracy: 0.3627\n",
      "Epoch 194/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0015 - Accuracy: 0.3721 - val_loss: 0.0011 - val_Accuracy: 0.3837\n",
      "Epoch 195/500\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 0.0012 - Accuracy: 0.3774 - val_loss: 0.0013 - val_Accuracy: 0.3561\n",
      "Epoch 196/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0017 - Accuracy: 0.3631 - val_loss: 0.0013 - val_Accuracy: 0.3811\n",
      "Epoch 197/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0012 - Accuracy: 0.3545 - val_loss: 0.0019 - val_Accuracy: 0.3443\n",
      "Epoch 198/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0015 - Accuracy: 0.3688 - val_loss: 0.0010 - val_Accuracy: 0.3548\n",
      "Epoch 199/500\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0015 - Accuracy: 0.3658 - val_loss: 0.0011 - val_Accuracy: 0.3666\n",
      "Epoch 200/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0012 - Accuracy: 0.3654 - val_loss: 0.0016 - val_Accuracy: 0.3640\n",
      "Epoch 201/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0016 - Accuracy: 0.3725 - val_loss: 0.0016 - val_Accuracy: 0.3653\n",
      "Epoch 202/500\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 0.0014 - Accuracy: 0.3658 - val_loss: 0.0015 - val_Accuracy: 0.4008\n",
      "Epoch 203/500\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 0.0014 - Accuracy: 0.3740 - val_loss: 0.0012 - val_Accuracy: 0.3535\n",
      "Epoch 204/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0016 - Accuracy: 0.3658 - val_loss: 0.0012 - val_Accuracy: 0.3627\n",
      "Epoch 205/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0012 - Accuracy: 0.3661 - val_loss: 0.0014 - val_Accuracy: 0.3890\n",
      "Epoch 206/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0014 - Accuracy: 0.3534 - val_loss: 0.0011 - val_Accuracy: 0.3311\n",
      "Epoch 207/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0013 - Accuracy: 0.3594 - val_loss: 0.0015 - val_Accuracy: 0.3666\n",
      "Epoch 208/500\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 0.0013 - Accuracy: 0.3658 - val_loss: 0.0012 - val_Accuracy: 0.3903\n",
      "Epoch 209/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0015 - Accuracy: 0.3676 - val_loss: 0.0016 - val_Accuracy: 0.3863\n",
      "Epoch 210/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0015 - Accuracy: 0.3616 - val_loss: 0.0019 - val_Accuracy: 0.3719\n",
      "Epoch 211/500\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 0.0016 - Accuracy: 0.3605 - val_loss: 0.0020 - val_Accuracy: 0.3758\n",
      "Epoch 212/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0014 - Accuracy: 0.3654 - val_loss: 0.0011 - val_Accuracy: 0.3784\n",
      "Epoch 213/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0019 - Accuracy: 0.3635 - val_loss: 0.0031 - val_Accuracy: 0.3758\n",
      "Epoch 214/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0014 - Accuracy: 0.3646 - val_loss: 0.0020 - val_Accuracy: 0.3719\n",
      "Epoch 215/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0019 - Accuracy: 0.3627 - val_loss: 0.0024 - val_Accuracy: 0.3417\n",
      "Epoch 216/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0014 - Accuracy: 0.3718 - val_loss: 0.0012 - val_Accuracy: 0.3876\n",
      "Epoch 217/500\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 0.0018 - Accuracy: 0.3658 - val_loss: 0.0029 - val_Accuracy: 0.3942\n",
      "Epoch 218/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0017 - Accuracy: 0.3710 - val_loss: 0.0015 - val_Accuracy: 0.3377\n",
      "Epoch 219/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0013 - Accuracy: 0.3620 - val_loss: 0.0013 - val_Accuracy: 0.4034\n",
      "Epoch 220/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0015 - Accuracy: 0.3612 - val_loss: 0.0012 - val_Accuracy: 0.3811\n",
      "Epoch 221/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0013 - Accuracy: 0.3612 - val_loss: 0.0010 - val_Accuracy: 0.3627\n",
      "Epoch 222/500\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0013 - Accuracy: 0.3643 - val_loss: 0.0013 - val_Accuracy: 0.3601\n",
      "Epoch 223/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0019 - Accuracy: 0.3627 - val_loss: 0.0039 - val_Accuracy: 0.3627\n",
      "Epoch 224/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0024 - Accuracy: 0.3530 - val_loss: 0.0018 - val_Accuracy: 0.3377\n",
      "Epoch 225/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0013 - Accuracy: 0.3597 - val_loss: 0.0012 - val_Accuracy: 0.3469\n",
      "Epoch 226/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0013 - Accuracy: 0.3601 - val_loss: 0.0014 - val_Accuracy: 0.3653\n",
      "Epoch 227/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0014 - Accuracy: 0.3564 - val_loss: 0.0018 - val_Accuracy: 0.3784\n",
      "Epoch 228/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0013 - Accuracy: 0.3620 - val_loss: 0.0012 - val_Accuracy: 0.3390\n",
      "Epoch 229/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0013 - Accuracy: 0.3624 - val_loss: 0.0013 - val_Accuracy: 0.3798\n",
      "Epoch 230/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0016 - Accuracy: 0.3643 - val_loss: 0.0012 - val_Accuracy: 0.3719\n",
      "Epoch 231/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0014 - Accuracy: 0.3639 - val_loss: 0.0025 - val_Accuracy: 0.3679\n",
      "Epoch 232/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0012 - Accuracy: 0.3601 - val_loss: 0.0011 - val_Accuracy: 0.3574\n",
      "Epoch 233/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0016 - Accuracy: 0.3605 - val_loss: 0.0025 - val_Accuracy: 0.3627\n",
      "Epoch 234/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0018 - Accuracy: 0.3631 - val_loss: 0.0012 - val_Accuracy: 0.3706\n",
      "Epoch 235/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0015 - Accuracy: 0.3699 - val_loss: 0.0010 - val_Accuracy: 0.3811\n",
      "Epoch 236/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0012 - Accuracy: 0.3654 - val_loss: 0.0011 - val_Accuracy: 0.3601\n",
      "Epoch 237/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0014 - Accuracy: 0.3590 - val_loss: 0.0040 - val_Accuracy: 0.3627\n",
      "Epoch 238/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0014 - Accuracy: 0.3522 - val_loss: 0.0015 - val_Accuracy: 0.3614\n",
      "Epoch 239/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0012 - Accuracy: 0.3601 - val_loss: 0.0014 - val_Accuracy: 0.3745\n",
      "Epoch 240/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0014 - Accuracy: 0.3556 - val_loss: 0.0014 - val_Accuracy: 0.3574\n",
      "Epoch 241/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0013 - Accuracy: 0.3601 - val_loss: 0.0013 - val_Accuracy: 0.3719\n",
      "Epoch 242/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0012 - Accuracy: 0.3545 - val_loss: 0.0021 - val_Accuracy: 0.3417\n",
      "Epoch 243/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0014 - Accuracy: 0.3582 - val_loss: 0.0019 - val_Accuracy: 0.3679\n",
      "Epoch 244/500\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 0.0018 - Accuracy: 0.3552 - val_loss: 0.0016 - val_Accuracy: 0.3495\n",
      "Epoch 245/500\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0014 - Accuracy: 0.3601 - val_loss: 0.0014 - val_Accuracy: 0.3495\n",
      "Epoch 246/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0014 - Accuracy: 0.3579 - val_loss: 0.0013 - val_Accuracy: 0.3482\n",
      "Epoch 247/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0012 - Accuracy: 0.3575 - val_loss: 0.0012 - val_Accuracy: 0.3601\n",
      "Epoch 248/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0014 - Accuracy: 0.3609 - val_loss: 0.0016 - val_Accuracy: 0.3679\n",
      "Epoch 249/500\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 0.0013 - Accuracy: 0.3631 - val_loss: 0.0014 - val_Accuracy: 0.3758\n",
      "Epoch 250/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0015 - Accuracy: 0.3556 - val_loss: 0.0018 - val_Accuracy: 0.3863\n",
      "Epoch 251/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0014 - Accuracy: 0.3560 - val_loss: 0.0023 - val_Accuracy: 0.3679\n",
      "Epoch 252/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0016 - Accuracy: 0.3646 - val_loss: 0.0013 - val_Accuracy: 0.3561\n",
      "Epoch 253/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0013 - Accuracy: 0.3571 - val_loss: 0.0014 - val_Accuracy: 0.3824\n",
      "Epoch 254/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0013 - Accuracy: 0.3579 - val_loss: 0.0012 - val_Accuracy: 0.3824\n",
      "Epoch 255/500\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0013 - Accuracy: 0.3654 - val_loss: 0.0012 - val_Accuracy: 0.3732\n",
      "Epoch 256/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0013 - Accuracy: 0.3605 - val_loss: 0.0014 - val_Accuracy: 0.3482\n",
      "Epoch 257/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0015 - Accuracy: 0.3627 - val_loss: 0.0018 - val_Accuracy: 0.3811\n",
      "Epoch 258/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0013 - Accuracy: 0.3590 - val_loss: 0.0015 - val_Accuracy: 0.3798\n",
      "Epoch 259/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0014 - Accuracy: 0.3571 - val_loss: 0.0011 - val_Accuracy: 0.3666\n",
      "Epoch 260/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0013 - Accuracy: 0.3571 - val_loss: 0.0015 - val_Accuracy: 0.3666\n",
      "Epoch 261/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0014 - Accuracy: 0.3590 - val_loss: 0.0011 - val_Accuracy: 0.3745\n",
      "Epoch 262/500\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0015 - Accuracy: 0.3549 - val_loss: 0.0012 - val_Accuracy: 0.3509\n",
      "Epoch 263/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0012 - Accuracy: 0.3590 - val_loss: 0.0013 - val_Accuracy: 0.3653\n",
      "Epoch 264/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0015 - Accuracy: 0.3534 - val_loss: 0.0017 - val_Accuracy: 0.3601\n",
      "Epoch 265/500\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0014 - Accuracy: 0.3579 - val_loss: 0.0012 - val_Accuracy: 0.3469\n",
      "Epoch 266/500\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0011 - Accuracy: 0.3567 - val_loss: 0.0012 - val_Accuracy: 0.3811\n",
      "Epoch 267/500\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0016 - Accuracy: 0.3571 - val_loss: 0.0013 - val_Accuracy: 0.3430\n",
      "Epoch 268/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0013 - Accuracy: 0.3624 - val_loss: 0.0015 - val_Accuracy: 0.3403\n",
      "Epoch 269/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0013 - Accuracy: 0.3601 - val_loss: 0.0014 - val_Accuracy: 0.3430\n",
      "Epoch 270/500\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0014 - Accuracy: 0.3582 - val_loss: 0.0018 - val_Accuracy: 0.3640\n",
      "Epoch 271/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0014 - Accuracy: 0.3586 - val_loss: 0.0014 - val_Accuracy: 0.3601\n",
      "Epoch 272/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0013 - Accuracy: 0.3616 - val_loss: 0.0035 - val_Accuracy: 0.3863\n",
      "Epoch 273/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0016 - Accuracy: 0.3582 - val_loss: 0.0014 - val_Accuracy: 0.3614\n",
      "Epoch 274/500\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0018 - Accuracy: 0.3545 - val_loss: 0.0012 - val_Accuracy: 0.3811\n",
      "Epoch 275/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0016 - Accuracy: 0.3620 - val_loss: 0.0012 - val_Accuracy: 0.3456\n",
      "Epoch 276/500\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0016 - Accuracy: 0.3504 - val_loss: 0.0030 - val_Accuracy: 0.3548\n",
      "Epoch 277/500\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0018 - Accuracy: 0.3590 - val_loss: 0.0013 - val_Accuracy: 0.3574\n",
      "Epoch 278/500\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0014 - Accuracy: 0.3571 - val_loss: 0.0013 - val_Accuracy: 0.3325\n",
      "Epoch 279/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0013 - Accuracy: 0.3635 - val_loss: 0.0018 - val_Accuracy: 0.3719\n",
      "Epoch 280/500\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0014 - Accuracy: 0.3582 - val_loss: 0.0010 - val_Accuracy: 0.3495\n",
      "Epoch 281/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0015 - Accuracy: 0.3541 - val_loss: 0.0011 - val_Accuracy: 0.3811\n",
      "Epoch 282/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0013 - Accuracy: 0.3605 - val_loss: 0.0014 - val_Accuracy: 0.3377\n",
      "Epoch 283/500\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0013 - Accuracy: 0.3537 - val_loss: 0.0022 - val_Accuracy: 0.3784\n",
      "Epoch 284/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0013 - Accuracy: 0.3564 - val_loss: 0.0014 - val_Accuracy: 0.3561\n",
      "Epoch 285/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0011 - Accuracy: 0.3597 - val_loss: 0.0016 - val_Accuracy: 0.3246\n",
      "Epoch 286/500\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0013 - Accuracy: 0.3537 - val_loss: 0.0012 - val_Accuracy: 0.3666\n",
      "Epoch 287/500\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0013 - Accuracy: 0.3545 - val_loss: 0.0012 - val_Accuracy: 0.3587\n",
      "Epoch 288/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0015 - Accuracy: 0.3601 - val_loss: 0.0028 - val_Accuracy: 0.3180\n",
      "Epoch 289/500\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0015 - Accuracy: 0.3496 - val_loss: 0.0020 - val_Accuracy: 0.3561\n",
      "Epoch 290/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0015 - Accuracy: 0.3549 - val_loss: 0.0015 - val_Accuracy: 0.3377\n",
      "Epoch 291/500\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0015 - Accuracy: 0.3466 - val_loss: 0.0013 - val_Accuracy: 0.3495\n",
      "Epoch 292/500\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0017 - Accuracy: 0.3522 - val_loss: 0.0017 - val_Accuracy: 0.3719\n",
      "Epoch 293/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0013 - Accuracy: 0.3526 - val_loss: 0.0015 - val_Accuracy: 0.3430\n",
      "Epoch 294/500\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.0013 - Accuracy: 0.3560 - val_loss: 0.0015 - val_Accuracy: 0.3693\n",
      "Epoch 295/500\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0013 - Accuracy: 0.3575 - val_loss: 0.0013 - val_Accuracy: 0.3666\n",
      "Epoch 296/500\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0013 - Accuracy: 0.3470 - val_loss: 0.0014 - val_Accuracy: 0.3522\n",
      "Epoch 297/500\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.0013 - Accuracy: 0.3556 - val_loss: 0.0017 - val_Accuracy: 0.3522\n",
      "Epoch 298/500\n",
      "84/84 [==============================] - 3s 31ms/step - loss: 0.0014 - Accuracy: 0.3526 - val_loss: 0.0018 - val_Accuracy: 0.3574\n",
      "Epoch 299/500\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.0012 - Accuracy: 0.3537 - val_loss: 0.0011 - val_Accuracy: 0.3311\n",
      "Epoch 300/500\n",
      "84/84 [==============================] - 3s 30ms/step - loss: 0.0013 - Accuracy: 0.3519 - val_loss: 0.0012 - val_Accuracy: 0.3601\n",
      "Epoch 301/500\n",
      "84/84 [==============================] - 3s 30ms/step - loss: 0.0013 - Accuracy: 0.3481 - val_loss: 0.0012 - val_Accuracy: 0.3601\n",
      "Epoch 302/500\n",
      "84/84 [==============================] - 2s 27ms/step - loss: 0.0012 - Accuracy: 0.3489 - val_loss: 0.0011 - val_Accuracy: 0.3614\n",
      "Epoch 303/500\n",
      "84/84 [==============================] - 3s 38ms/step - loss: 0.0014 - Accuracy: 0.3511 - val_loss: 0.0018 - val_Accuracy: 0.3679\n",
      "Epoch 304/500\n",
      "84/84 [==============================] - 3s 34ms/step - loss: 0.0014 - Accuracy: 0.3507 - val_loss: 0.0017 - val_Accuracy: 0.3417\n",
      "Epoch 305/500\n",
      "84/84 [==============================] - 3s 34ms/step - loss: 0.0012 - Accuracy: 0.3567 - val_loss: 0.0017 - val_Accuracy: 0.3627\n",
      "Epoch 306/500\n",
      "84/84 [==============================] - 3s 31ms/step - loss: 0.0014 - Accuracy: 0.3534 - val_loss: 0.0011 - val_Accuracy: 0.3693\n",
      "Epoch 307/500\n",
      "84/84 [==============================] - 3s 35ms/step - loss: 0.0015 - Accuracy: 0.3519 - val_loss: 0.0026 - val_Accuracy: 0.3640\n",
      "Epoch 308/500\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.0015 - Accuracy: 0.3504 - val_loss: 0.0011 - val_Accuracy: 0.3535\n",
      "Epoch 309/500\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.0012 - Accuracy: 0.3507 - val_loss: 0.0016 - val_Accuracy: 0.3706\n",
      "Epoch 310/500\n",
      "84/84 [==============================] - 4s 44ms/step - loss: 0.0012 - Accuracy: 0.3496 - val_loss: 0.0010 - val_Accuracy: 0.3509\n",
      "Epoch 311/500\n",
      "84/84 [==============================] - 3s 31ms/step - loss: 0.0013 - Accuracy: 0.3462 - val_loss: 0.0017 - val_Accuracy: 0.3443\n",
      "Epoch 312/500\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.0013 - Accuracy: 0.3515 - val_loss: 0.0020 - val_Accuracy: 0.3417\n",
      "Epoch 313/500\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.0014 - Accuracy: 0.3541 - val_loss: 0.0012 - val_Accuracy: 0.3561\n",
      "Epoch 314/500\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0012 - Accuracy: 0.3519 - val_loss: 0.0013 - val_Accuracy: 0.3443\n",
      "Epoch 315/500\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0013 - Accuracy: 0.3492 - val_loss: 0.0018 - val_Accuracy: 0.3706\n",
      "Epoch 316/500\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.0014 - Accuracy: 0.3545 - val_loss: 0.0027 - val_Accuracy: 0.3417\n",
      "Epoch 317/500\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.0016 - Accuracy: 0.3466 - val_loss: 0.0019 - val_Accuracy: 0.3548\n",
      "Epoch 318/500\n",
      "84/84 [==============================] - 2s 27ms/step - loss: 0.0012 - Accuracy: 0.3489 - val_loss: 0.0021 - val_Accuracy: 0.3456\n",
      "Epoch 319/500\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.0018 - Accuracy: 0.3511 - val_loss: 0.0017 - val_Accuracy: 0.3614\n",
      "Epoch 320/500\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0013 - Accuracy: 0.3455 - val_loss: 0.0013 - val_Accuracy: 0.3548\n",
      "Epoch 321/500\n",
      "84/84 [==============================] - 2s 28ms/step - loss: 0.0013 - Accuracy: 0.3560 - val_loss: 0.0013 - val_Accuracy: 0.3311\n",
      "Epoch 322/500\n",
      "84/84 [==============================] - 2s 29ms/step - loss: 0.0012 - Accuracy: 0.3492 - val_loss: 0.0012 - val_Accuracy: 0.3798\n",
      "Epoch 323/500\n",
      "84/84 [==============================] - 2s 30ms/step - loss: 0.0013 - Accuracy: 0.3597 - val_loss: 0.0018 - val_Accuracy: 0.3535\n",
      "Epoch 324/500\n",
      "84/84 [==============================] - 3s 30ms/step - loss: 0.0012 - Accuracy: 0.3534 - val_loss: 0.0033 - val_Accuracy: 0.3587\n",
      "Epoch 325/500\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.0013 - Accuracy: 0.3560 - val_loss: 0.0013 - val_Accuracy: 0.3627\n",
      "Epoch 326/500\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0015 - Accuracy: 0.3545 - val_loss: 0.0019 - val_Accuracy: 0.3509\n",
      "Epoch 327/500\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.0012 - Accuracy: 0.3515 - val_loss: 0.0021 - val_Accuracy: 0.3443\n",
      "Epoch 328/500\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0015 - Accuracy: 0.3586 - val_loss: 0.0018 - val_Accuracy: 0.3798\n",
      "Epoch 329/500\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.0013 - Accuracy: 0.3530 - val_loss: 0.0013 - val_Accuracy: 0.3601\n",
      "Epoch 330/500\n",
      "84/84 [==============================] - 3s 31ms/step - loss: 0.0013 - Accuracy: 0.3500 - val_loss: 0.0020 - val_Accuracy: 0.3390\n",
      "Epoch 331/500\n",
      "84/84 [==============================] - 3s 37ms/step - loss: 0.0016 - Accuracy: 0.3549 - val_loss: 0.0015 - val_Accuracy: 0.3666\n",
      "Epoch 332/500\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.0013 - Accuracy: 0.3549 - val_loss: 0.0024 - val_Accuracy: 0.3706\n",
      "Epoch 333/500\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.0015 - Accuracy: 0.3541 - val_loss: 0.0023 - val_Accuracy: 0.3390\n",
      "Epoch 334/500\n",
      "84/84 [==============================] - 2s 29ms/step - loss: 0.0014 - Accuracy: 0.3466 - val_loss: 0.0015 - val_Accuracy: 0.3535\n",
      "Epoch 335/500\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.0013 - Accuracy: 0.3447 - val_loss: 0.0012 - val_Accuracy: 0.3522\n",
      "Epoch 336/500\n",
      "84/84 [==============================] - 2s 27ms/step - loss: 0.0011 - Accuracy: 0.3489 - val_loss: 0.0012 - val_Accuracy: 0.3469\n",
      "Epoch 337/500\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0011 - Accuracy: 0.3451 - val_loss: 0.0013 - val_Accuracy: 0.3443\n",
      "Epoch 338/500\n",
      "84/84 [==============================] - 2s 29ms/step - loss: 0.0013 - Accuracy: 0.3489 - val_loss: 0.0018 - val_Accuracy: 0.3653\n",
      "Epoch 339/500\n",
      "84/84 [==============================] - 2s 27ms/step - loss: 0.0016 - Accuracy: 0.3556 - val_loss: 0.0013 - val_Accuracy: 0.3325\n",
      "Epoch 340/500\n",
      "84/84 [==============================] - 2s 27ms/step - loss: 0.0015 - Accuracy: 0.3515 - val_loss: 0.0017 - val_Accuracy: 0.3075\n",
      "Epoch 341/500\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.0012 - Accuracy: 0.3492 - val_loss: 0.0017 - val_Accuracy: 0.3693\n",
      "Epoch 342/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0014 - Accuracy: 0.3492 - val_loss: 0.0038 - val_Accuracy: 0.3403\n",
      "Epoch 343/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0015 - Accuracy: 0.3447 - val_loss: 0.0013 - val_Accuracy: 0.3233\n",
      "Epoch 344/500\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0013 - Accuracy: 0.3489 - val_loss: 0.0023 - val_Accuracy: 0.3522\n",
      "Epoch 345/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0014 - Accuracy: 0.3443 - val_loss: 0.0012 - val_Accuracy: 0.3706\n",
      "Epoch 346/500\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0014 - Accuracy: 0.3489 - val_loss: 0.0012 - val_Accuracy: 0.3561\n",
      "Epoch 347/500\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0012 - Accuracy: 0.3474 - val_loss: 0.0014 - val_Accuracy: 0.3456\n",
      "Epoch 348/500\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0012 - Accuracy: 0.3522 - val_loss: 0.0014 - val_Accuracy: 0.3403\n",
      "Epoch 349/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0012 - Accuracy: 0.3440 - val_loss: 0.0018 - val_Accuracy: 0.3561\n",
      "Epoch 350/500\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0011 - Accuracy: 0.3485 - val_loss: 0.0010 - val_Accuracy: 0.3364\n",
      "Epoch 351/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0013 - Accuracy: 0.3511 - val_loss: 0.0011 - val_Accuracy: 0.3311\n",
      "Epoch 352/500\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0012 - Accuracy: 0.3496 - val_loss: 0.0012 - val_Accuracy: 0.3114\n",
      "Epoch 353/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0015 - Accuracy: 0.3496 - val_loss: 0.0014 - val_Accuracy: 0.3601\n",
      "Epoch 354/500\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0012 - Accuracy: 0.3511 - val_loss: 0.0020 - val_Accuracy: 0.3535\n",
      "Epoch 355/500\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0013 - Accuracy: 0.3519 - val_loss: 0.0011 - val_Accuracy: 0.3403\n",
      "Epoch 356/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0013 - Accuracy: 0.3421 - val_loss: 0.0020 - val_Accuracy: 0.3430\n",
      "Epoch 357/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0012 - Accuracy: 0.3526 - val_loss: 0.0014 - val_Accuracy: 0.3758\n",
      "Epoch 358/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0012 - Accuracy: 0.3507 - val_loss: 0.0011 - val_Accuracy: 0.3719\n",
      "Epoch 359/500\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0012 - Accuracy: 0.3492 - val_loss: 0.0014 - val_Accuracy: 0.3430\n",
      "Epoch 360/500\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0014 - Accuracy: 0.3526 - val_loss: 0.0014 - val_Accuracy: 0.3364\n",
      "Epoch 361/500\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.0013 - Accuracy: 0.3485 - val_loss: 0.0012 - val_Accuracy: 0.3456\n",
      "Epoch 362/500\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.0047 - Accuracy: 0.3496 - val_loss: 0.0033 - val_Accuracy: 0.3890\n",
      "Epoch 363/500\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.0036 - Accuracy: 0.3406 - val_loss: 0.0026 - val_Accuracy: 0.3351\n",
      "Epoch 364/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0018 - Accuracy: 0.3395 - val_loss: 0.0018 - val_Accuracy: 0.3469\n",
      "Epoch 365/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0017 - Accuracy: 0.3485 - val_loss: 0.0015 - val_Accuracy: 0.3062\n",
      "Epoch 366/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0017 - Accuracy: 0.3432 - val_loss: 0.0013 - val_Accuracy: 0.3417\n",
      "Epoch 367/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0018 - Accuracy: 0.3466 - val_loss: 0.0013 - val_Accuracy: 0.3338\n",
      "Epoch 368/500\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0014 - Accuracy: 0.3459 - val_loss: 0.0016 - val_Accuracy: 0.3601\n",
      "Epoch 369/500\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0012 - Accuracy: 0.3477 - val_loss: 0.0018 - val_Accuracy: 0.3614\n",
      "Epoch 370/500\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0015 - Accuracy: 0.3560 - val_loss: 0.0018 - val_Accuracy: 0.3679\n",
      "Epoch 371/500\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0015 - Accuracy: 0.3590 - val_loss: 0.0015 - val_Accuracy: 0.3548\n",
      "Epoch 372/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0013 - Accuracy: 0.3515 - val_loss: 0.0011 - val_Accuracy: 0.3693\n",
      "Epoch 373/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0013 - Accuracy: 0.3579 - val_loss: 0.0012 - val_Accuracy: 0.3482\n",
      "Epoch 374/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0012 - Accuracy: 0.3556 - val_loss: 0.0013 - val_Accuracy: 0.3719\n",
      "Epoch 375/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0014 - Accuracy: 0.3575 - val_loss: 0.0015 - val_Accuracy: 0.3640\n",
      "Epoch 376/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0013 - Accuracy: 0.3567 - val_loss: 0.0013 - val_Accuracy: 0.3417\n",
      "Epoch 377/500\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0011 - Accuracy: 0.3601 - val_loss: 0.0020 - val_Accuracy: 0.3784\n",
      "Epoch 378/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0015 - Accuracy: 0.3571 - val_loss: 0.0018 - val_Accuracy: 0.3456\n",
      "Epoch 379/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0013 - Accuracy: 0.3575 - val_loss: 0.0013 - val_Accuracy: 0.3561\n",
      "Epoch 380/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0011 - Accuracy: 0.3507 - val_loss: 0.0013 - val_Accuracy: 0.3469\n",
      "Epoch 381/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0012 - Accuracy: 0.3575 - val_loss: 0.0012 - val_Accuracy: 0.3666\n",
      "Epoch 382/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0014 - Accuracy: 0.3552 - val_loss: 0.0012 - val_Accuracy: 0.3430\n",
      "Epoch 383/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0013 - Accuracy: 0.3541 - val_loss: 0.0012 - val_Accuracy: 0.3574\n",
      "Epoch 384/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0012 - Accuracy: 0.3511 - val_loss: 0.0017 - val_Accuracy: 0.3640\n",
      "Epoch 385/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0012 - Accuracy: 0.3575 - val_loss: 0.0013 - val_Accuracy: 0.3640\n",
      "Epoch 386/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0013 - Accuracy: 0.3567 - val_loss: 0.0012 - val_Accuracy: 0.3640\n",
      "Epoch 387/500\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0012 - Accuracy: 0.3492 - val_loss: 0.0019 - val_Accuracy: 0.3627\n",
      "Epoch 388/500\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0017 - Accuracy: 0.3496 - val_loss: 0.0014 - val_Accuracy: 0.3890\n",
      "Epoch 389/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0014 - Accuracy: 0.3586 - val_loss: 0.0015 - val_Accuracy: 0.3693\n",
      "Epoch 390/500\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0013 - Accuracy: 0.3522 - val_loss: 0.0013 - val_Accuracy: 0.3417\n",
      "Epoch 391/500\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 0.0014 - Accuracy: 0.3597 - val_loss: 0.0012 - val_Accuracy: 0.3640\n",
      "Epoch 392/500\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 0.0013 - Accuracy: 0.3560 - val_loss: 0.0012 - val_Accuracy: 0.3666\n",
      "Epoch 393/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0013 - Accuracy: 0.3474 - val_loss: 0.0014 - val_Accuracy: 0.3640\n",
      "Epoch 394/500\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 0.0013 - Accuracy: 0.3507 - val_loss: 0.0016 - val_Accuracy: 0.3601\n",
      "Epoch 395/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0014 - Accuracy: 0.3549 - val_loss: 0.0013 - val_Accuracy: 0.3522\n",
      "Epoch 396/500\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 0.0013 - Accuracy: 0.3474 - val_loss: 0.0011 - val_Accuracy: 0.3495\n",
      "Epoch 397/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0012 - Accuracy: 0.3436 - val_loss: 0.0011 - val_Accuracy: 0.3771\n",
      "Epoch 398/500\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 0.0013 - Accuracy: 0.3594 - val_loss: 0.0011 - val_Accuracy: 0.3417\n",
      "Epoch 399/500\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 0.0013 - Accuracy: 0.3530 - val_loss: 0.0018 - val_Accuracy: 0.3456\n",
      "Epoch 400/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0012 - Accuracy: 0.3545 - val_loss: 0.0012 - val_Accuracy: 0.3548\n",
      "Epoch 401/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0016 - Accuracy: 0.3530 - val_loss: 0.0016 - val_Accuracy: 0.3509\n",
      "Epoch 402/500\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 0.0014 - Accuracy: 0.3485 - val_loss: 0.0016 - val_Accuracy: 0.3509\n",
      "Epoch 403/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0012 - Accuracy: 0.3586 - val_loss: 0.0013 - val_Accuracy: 0.3377\n",
      "Epoch 404/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0013 - Accuracy: 0.3534 - val_loss: 0.0018 - val_Accuracy: 0.3640\n",
      "Epoch 405/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0012 - Accuracy: 0.3519 - val_loss: 0.0011 - val_Accuracy: 0.3522\n",
      "Epoch 406/500\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 0.0012 - Accuracy: 0.3609 - val_loss: 0.0014 - val_Accuracy: 0.3443\n",
      "Epoch 407/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0014 - Accuracy: 0.3519 - val_loss: 9.8999e-04 - val_Accuracy: 0.3509\n",
      "Epoch 408/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0011 - Accuracy: 0.3552 - val_loss: 0.0013 - val_Accuracy: 0.3390\n",
      "Epoch 409/500\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 0.0013 - Accuracy: 0.3579 - val_loss: 0.0014 - val_Accuracy: 0.3666\n",
      "Epoch 410/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0014 - Accuracy: 0.3500 - val_loss: 0.0017 - val_Accuracy: 0.3798\n",
      "Epoch 411/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0014 - Accuracy: 0.3534 - val_loss: 0.0013 - val_Accuracy: 0.3443\n",
      "Epoch 412/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0012 - Accuracy: 0.3594 - val_loss: 0.0012 - val_Accuracy: 0.3758\n",
      "Epoch 413/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0013 - Accuracy: 0.3507 - val_loss: 0.0024 - val_Accuracy: 0.3693\n",
      "Epoch 414/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0015 - Accuracy: 0.3519 - val_loss: 0.0013 - val_Accuracy: 0.3469\n",
      "Epoch 415/500\n",
      "84/84 [==============================] - 1s 14ms/step - loss: 0.0012 - Accuracy: 0.3522 - val_loss: 0.0021 - val_Accuracy: 0.3351\n",
      "Epoch 416/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 0.0013 - Accuracy: 0.3500 - val_loss: 0.0017 - val_Accuracy: 0.3167\n",
      "Epoch 417/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0015 - Accuracy: 0.3537 - val_loss: 0.0021 - val_Accuracy: 0.3561\n",
      "Epoch 418/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0013 - Accuracy: 0.3511 - val_loss: 0.0011 - val_Accuracy: 0.3601\n",
      "Epoch 419/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0013 - Accuracy: 0.3496 - val_loss: 0.0012 - val_Accuracy: 0.3679\n",
      "Epoch 420/500\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0013 - Accuracy: 0.3631 - val_loss: 0.0020 - val_Accuracy: 0.2917\n",
      "Epoch 421/500\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.0014 - Accuracy: 0.3564 - val_loss: 0.0012 - val_Accuracy: 0.3285\n",
      "Epoch 422/500\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0013 - Accuracy: 0.3560 - val_loss: 0.0017 - val_Accuracy: 0.3719\n",
      "Epoch 423/500\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0014 - Accuracy: 0.3522 - val_loss: 0.0013 - val_Accuracy: 0.3706\n",
      "Epoch 424/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0014 - Accuracy: 0.3616 - val_loss: 0.0011 - val_Accuracy: 0.3653\n",
      "Epoch 425/500\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0012 - Accuracy: 0.3507 - val_loss: 0.0015 - val_Accuracy: 0.3482\n",
      "Epoch 426/500\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0013 - Accuracy: 0.3567 - val_loss: 0.0014 - val_Accuracy: 0.3246\n",
      "Epoch 427/500\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0013 - Accuracy: 0.3560 - val_loss: 0.0013 - val_Accuracy: 0.3509\n",
      "Epoch 428/500\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0012 - Accuracy: 0.3526 - val_loss: 0.0010 - val_Accuracy: 0.3430\n",
      "Epoch 429/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0017 - Accuracy: 0.3597 - val_loss: 0.0023 - val_Accuracy: 0.3285\n",
      "Epoch 430/500\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0014 - Accuracy: 0.3515 - val_loss: 0.0013 - val_Accuracy: 0.3311\n",
      "Epoch 431/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0012 - Accuracy: 0.3541 - val_loss: 0.0013 - val_Accuracy: 0.3167\n",
      "Epoch 432/500\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.0014 - Accuracy: 0.3586 - val_loss: 0.0011 - val_Accuracy: 0.3535\n",
      "Epoch 433/500\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0014 - Accuracy: 0.3556 - val_loss: 0.0013 - val_Accuracy: 0.3561\n",
      "Epoch 434/500\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0015 - Accuracy: 0.3575 - val_loss: 0.0013 - val_Accuracy: 0.3745\n",
      "Epoch 435/500\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.0013 - Accuracy: 0.3519 - val_loss: 0.0018 - val_Accuracy: 0.3390\n",
      "Epoch 436/500\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0016 - Accuracy: 0.3530 - val_loss: 0.0010 - val_Accuracy: 0.3443\n",
      "Epoch 437/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0014 - Accuracy: 0.3545 - val_loss: 0.0012 - val_Accuracy: 0.3679\n",
      "Epoch 438/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0013 - Accuracy: 0.3500 - val_loss: 0.0011 - val_Accuracy: 0.3601\n",
      "Epoch 439/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0013 - Accuracy: 0.3522 - val_loss: 0.0014 - val_Accuracy: 0.3693\n",
      "Epoch 440/500\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0013 - Accuracy: 0.3545 - val_loss: 0.0013 - val_Accuracy: 0.3587\n",
      "Epoch 441/500\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.0014 - Accuracy: 0.3534 - val_loss: 0.0016 - val_Accuracy: 0.3574\n",
      "Epoch 442/500\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.0012 - Accuracy: 0.3643 - val_loss: 0.0013 - val_Accuracy: 0.3495\n",
      "Epoch 443/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0014 - Accuracy: 0.3515 - val_loss: 0.0011 - val_Accuracy: 0.3693\n",
      "Epoch 444/500\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0012 - Accuracy: 0.3564 - val_loss: 0.0017 - val_Accuracy: 0.3482\n",
      "Epoch 445/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0014 - Accuracy: 0.3567 - val_loss: 0.0021 - val_Accuracy: 0.3469\n",
      "Epoch 446/500\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0013 - Accuracy: 0.3552 - val_loss: 0.0010 - val_Accuracy: 0.3601\n",
      "Epoch 447/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0013 - Accuracy: 0.3582 - val_loss: 0.0017 - val_Accuracy: 0.3469\n",
      "Epoch 448/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0013 - Accuracy: 0.3534 - val_loss: 0.0017 - val_Accuracy: 0.3640\n",
      "Epoch 449/500\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0016 - Accuracy: 0.3496 - val_loss: 0.0015 - val_Accuracy: 0.3850\n",
      "Epoch 450/500\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0015 - Accuracy: 0.3556 - val_loss: 0.0013 - val_Accuracy: 0.3732\n",
      "Epoch 451/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0012 - Accuracy: 0.3594 - val_loss: 0.0014 - val_Accuracy: 0.3574\n",
      "Epoch 452/500\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0011 - Accuracy: 0.3627 - val_loss: 0.0012 - val_Accuracy: 0.3456\n",
      "Epoch 453/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0012 - Accuracy: 0.3643 - val_loss: 0.0012 - val_Accuracy: 0.3495\n",
      "Epoch 454/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0012 - Accuracy: 0.3556 - val_loss: 0.0036 - val_Accuracy: 0.3430\n",
      "Epoch 455/500\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0027 - Accuracy: 0.3440 - val_loss: 0.0019 - val_Accuracy: 0.3298\n",
      "Epoch 456/500\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 0.0013 - Accuracy: 0.3530 - val_loss: 0.0014 - val_Accuracy: 0.3522\n",
      "Epoch 457/500\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0014 - Accuracy: 0.3552 - val_loss: 0.0014 - val_Accuracy: 0.3482\n",
      "Epoch 458/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0013 - Accuracy: 0.3601 - val_loss: 0.0011 - val_Accuracy: 0.3614\n",
      "Epoch 459/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0012 - Accuracy: 0.3594 - val_loss: 0.0012 - val_Accuracy: 0.3640\n",
      "Epoch 460/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0012 - Accuracy: 0.3522 - val_loss: 0.0013 - val_Accuracy: 0.3351\n",
      "Epoch 461/500\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.0013 - Accuracy: 0.3579 - val_loss: 0.0011 - val_Accuracy: 0.3377\n",
      "Epoch 462/500\n",
      "84/84 [==============================] - 3s 31ms/step - loss: 0.0013 - Accuracy: 0.3489 - val_loss: 0.0013 - val_Accuracy: 0.3443\n",
      "Epoch 463/500\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.0012 - Accuracy: 0.3571 - val_loss: 0.0025 - val_Accuracy: 0.3771\n",
      "Epoch 464/500\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.0019 - Accuracy: 0.3571 - val_loss: 0.0013 - val_Accuracy: 0.3653\n",
      "Epoch 465/500\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.0013 - Accuracy: 0.3571 - val_loss: 0.0015 - val_Accuracy: 0.3390\n",
      "Epoch 466/500\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.0013 - Accuracy: 0.3534 - val_loss: 0.0012 - val_Accuracy: 0.3443\n",
      "Epoch 467/500\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.0011 - Accuracy: 0.3601 - val_loss: 0.0018 - val_Accuracy: 0.3495\n",
      "Epoch 468/500\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0013 - Accuracy: 0.3579 - val_loss: 9.8109e-04 - val_Accuracy: 0.3601\n",
      "Epoch 469/500\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0011 - Accuracy: 0.3545 - val_loss: 0.0018 - val_Accuracy: 0.3601\n",
      "Epoch 470/500\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.0013 - Accuracy: 0.3556 - val_loss: 0.0012 - val_Accuracy: 0.3101\n",
      "Epoch 471/500\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.0012 - Accuracy: 0.3526 - val_loss: 0.0017 - val_Accuracy: 0.3784\n",
      "Epoch 472/500\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.0013 - Accuracy: 0.3579 - val_loss: 0.0018 - val_Accuracy: 0.3693\n",
      "Epoch 473/500\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.0015 - Accuracy: 0.3597 - val_loss: 0.0010 - val_Accuracy: 0.3561\n",
      "Epoch 474/500\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.0013 - Accuracy: 0.3571 - val_loss: 0.0017 - val_Accuracy: 0.3535\n",
      "Epoch 475/500\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.0014 - Accuracy: 0.3586 - val_loss: 0.0012 - val_Accuracy: 0.3246\n",
      "Epoch 476/500\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.0012 - Accuracy: 0.3500 - val_loss: 0.0017 - val_Accuracy: 0.3561\n",
      "Epoch 477/500\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.0012 - Accuracy: 0.3609 - val_loss: 0.0012 - val_Accuracy: 0.3574\n",
      "Epoch 478/500\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.0015 - Accuracy: 0.3612 - val_loss: 0.0014 - val_Accuracy: 0.3548\n",
      "Epoch 479/500\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.0013 - Accuracy: 0.3560 - val_loss: 0.0013 - val_Accuracy: 0.3469\n",
      "Epoch 480/500\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.0015 - Accuracy: 0.3624 - val_loss: 0.0012 - val_Accuracy: 0.3522\n",
      "Epoch 481/500\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.0013 - Accuracy: 0.3605 - val_loss: 0.0017 - val_Accuracy: 0.3535\n",
      "Epoch 482/500\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.0014 - Accuracy: 0.3556 - val_loss: 0.0015 - val_Accuracy: 0.3390\n",
      "Epoch 483/500\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.0015 - Accuracy: 0.3627 - val_loss: 0.0015 - val_Accuracy: 0.3509\n",
      "Epoch 484/500\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.0012 - Accuracy: 0.3605 - val_loss: 0.0012 - val_Accuracy: 0.3719\n",
      "Epoch 485/500\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.0013 - Accuracy: 0.3579 - val_loss: 0.0015 - val_Accuracy: 0.3627\n",
      "Epoch 486/500\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.0014 - Accuracy: 0.3564 - val_loss: 0.0015 - val_Accuracy: 0.3732\n",
      "Epoch 487/500\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.0014 - Accuracy: 0.3534 - val_loss: 0.0018 - val_Accuracy: 0.3627\n",
      "Epoch 488/500\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.0012 - Accuracy: 0.3650 - val_loss: 0.0014 - val_Accuracy: 0.3587\n",
      "Epoch 489/500\n",
      "84/84 [==============================] - 2s 27ms/step - loss: 0.0013 - Accuracy: 0.3522 - val_loss: 0.0015 - val_Accuracy: 0.3758\n",
      "Epoch 490/500\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.0013 - Accuracy: 0.3537 - val_loss: 0.0013 - val_Accuracy: 0.3601\n",
      "Epoch 491/500\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.0016 - Accuracy: 0.3620 - val_loss: 0.0012 - val_Accuracy: 0.3719\n",
      "Epoch 492/500\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.0014 - Accuracy: 0.3552 - val_loss: 0.0012 - val_Accuracy: 0.3693\n",
      "Epoch 493/500\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.0015 - Accuracy: 0.3571 - val_loss: 0.0010 - val_Accuracy: 0.3456\n",
      "Epoch 494/500\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.0011 - Accuracy: 0.3545 - val_loss: 0.0016 - val_Accuracy: 0.3745\n",
      "Epoch 495/500\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.0013 - Accuracy: 0.3582 - val_loss: 0.0013 - val_Accuracy: 0.3311\n",
      "Epoch 496/500\n",
      "84/84 [==============================] - 2s 27ms/step - loss: 0.0013 - Accuracy: 0.3564 - val_loss: 0.0013 - val_Accuracy: 0.3732\n",
      "Epoch 497/500\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.0015 - Accuracy: 0.3654 - val_loss: 0.0013 - val_Accuracy: 0.3706\n",
      "Epoch 498/500\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.0012 - Accuracy: 0.3571 - val_loss: 0.0014 - val_Accuracy: 0.3640\n",
      "Epoch 499/500\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.0014 - Accuracy: 0.3537 - val_loss: 0.0011 - val_Accuracy: 0.3614\n",
      "Epoch 500/500\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.0012 - Accuracy: 0.3575 - val_loss: 0.0010 - val_Accuracy: 0.3666\n"
     ]
    }
   ],
   "source": [
    "historyT = model.fit(X_train_scaled, y_train_scaled, validation_data=(X_val_scaled, y_val_scaled),epochs=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0025 - Accuracy: 0.3346 - val_loss: 0.0015 - val_Accuracy: 0.3403\n",
      "Epoch 2/500\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 0.0014 - Accuracy: 0.3395 - val_loss: 0.0012 - val_Accuracy: 0.3325\n",
      "Epoch 3/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.0015 - Accuracy: 0.3398 - val_loss: 0.0015 - val_Accuracy: 0.3561\n",
      "Epoch 4/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 0.0012 - Accuracy: 0.3417 - val_loss: 0.0014 - val_Accuracy: 0.3679\n",
      "Epoch 5/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0012 - Accuracy: 0.3425 - val_loss: 0.0012 - val_Accuracy: 0.3548\n",
      "Epoch 6/500\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 6.6797e-04 - Accuracy: 0.3511 - val_loss: 4.6236e-04 - val_Accuracy: 0.3509\n",
      "Epoch 7/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 5.0006e-04 - Accuracy: 0.3549 - val_loss: 7.0674e-04 - val_Accuracy: 0.3495\n",
      "Epoch 8/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 7.1574e-04 - Accuracy: 0.3530 - val_loss: 5.1323e-04 - val_Accuracy: 0.3653\n",
      "Epoch 9/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 6.4982e-04 - Accuracy: 0.3545 - val_loss: 5.5932e-04 - val_Accuracy: 0.3706\n",
      "Epoch 10/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 6.5697e-04 - Accuracy: 0.3582 - val_loss: 7.6763e-04 - val_Accuracy: 0.3745\n",
      "Epoch 11/500\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 6.2645e-04 - Accuracy: 0.3507 - val_loss: 4.3393e-04 - val_Accuracy: 0.3679\n",
      "Epoch 12/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 5.5974e-04 - Accuracy: 0.3522 - val_loss: 0.0015 - val_Accuracy: 0.3482\n",
      "Epoch 13/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 9.8637e-04 - Accuracy: 0.3492 - val_loss: 5.9290e-04 - val_Accuracy: 0.3430\n",
      "Epoch 14/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 6.6436e-04 - Accuracy: 0.3530 - val_loss: 6.8742e-04 - val_Accuracy: 0.3574\n",
      "Epoch 15/500\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 5.5580e-04 - Accuracy: 0.3500 - val_loss: 4.1898e-04 - val_Accuracy: 0.3798\n",
      "Epoch 16/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 6.1972e-04 - Accuracy: 0.3534 - val_loss: 8.5009e-04 - val_Accuracy: 0.3535\n",
      "Epoch 17/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 5.7594e-04 - Accuracy: 0.3579 - val_loss: 0.0013 - val_Accuracy: 0.3679\n",
      "Epoch 18/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 8.5333e-04 - Accuracy: 0.3549 - val_loss: 8.2078e-04 - val_Accuracy: 0.3272\n",
      "Epoch 19/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 7.1221e-04 - Accuracy: 0.3511 - val_loss: 6.1405e-04 - val_Accuracy: 0.3482\n",
      "Epoch 20/500\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 7.7935e-04 - Accuracy: 0.3515 - val_loss: 9.3152e-04 - val_Accuracy: 0.3811\n",
      "Epoch 21/500\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 5.5292e-04 - Accuracy: 0.3601 - val_loss: 4.8781e-04 - val_Accuracy: 0.3601\n",
      "Epoch 22/500\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 4.6453e-04 - Accuracy: 0.3549 - val_loss: 3.4849e-04 - val_Accuracy: 0.3693\n",
      "Epoch 23/500\n",
      "84/84 [==============================] - 2s 27ms/step - loss: 5.7159e-04 - Accuracy: 0.3612 - val_loss: 4.0360e-04 - val_Accuracy: 0.3653\n",
      "Epoch 24/500\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 4.5913e-04 - Accuracy: 0.3571 - val_loss: 4.5496e-04 - val_Accuracy: 0.3482\n",
      "Epoch 25/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 5.7070e-04 - Accuracy: 0.3627 - val_loss: 4.8645e-04 - val_Accuracy: 0.3587\n",
      "Epoch 26/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 6.0647e-04 - Accuracy: 0.3556 - val_loss: 4.1811e-04 - val_Accuracy: 0.3850\n",
      "Epoch 27/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 5.1816e-04 - Accuracy: 0.3537 - val_loss: 0.0012 - val_Accuracy: 0.3693\n",
      "Epoch 28/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 4.6329e-04 - Accuracy: 0.3586 - val_loss: 0.0032 - val_Accuracy: 0.3311\n",
      "Epoch 29/500\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 8.8253e-04 - Accuracy: 0.3567 - val_loss: 4.6454e-04 - val_Accuracy: 0.3758\n",
      "Epoch 30/500\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 5.1586e-04 - Accuracy: 0.3530 - val_loss: 5.2999e-04 - val_Accuracy: 0.3548\n",
      "Epoch 31/500\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 5.0899e-04 - Accuracy: 0.3643 - val_loss: 0.0013 - val_Accuracy: 0.3798\n",
      "Epoch 32/500\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 4.6549e-04 - Accuracy: 0.3560 - val_loss: 4.4426e-04 - val_Accuracy: 0.3745\n",
      "Epoch 33/500\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 4.6450e-04 - Accuracy: 0.3631 - val_loss: 0.0012 - val_Accuracy: 0.3719\n",
      "Epoch 34/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 4.7843e-04 - Accuracy: 0.3684 - val_loss: 4.5959e-04 - val_Accuracy: 0.3351\n",
      "Epoch 35/500\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 4.4314e-04 - Accuracy: 0.3624 - val_loss: 6.3780e-04 - val_Accuracy: 0.3587\n",
      "Epoch 36/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 5.1353e-04 - Accuracy: 0.3609 - val_loss: 3.5421e-04 - val_Accuracy: 0.3640\n",
      "Epoch 37/500\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 6.5489e-04 - Accuracy: 0.3545 - val_loss: 5.6538e-04 - val_Accuracy: 0.3522\n",
      "Epoch 38/500\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 4.6512e-04 - Accuracy: 0.3481 - val_loss: 4.7494e-04 - val_Accuracy: 0.3811\n",
      "Epoch 39/500\n",
      "84/84 [==============================] - 1s 18ms/step - loss: 4.7561e-04 - Accuracy: 0.3605 - val_loss: 3.4680e-04 - val_Accuracy: 0.3732\n",
      "Epoch 40/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 5.0983e-04 - Accuracy: 0.3564 - val_loss: 4.9711e-04 - val_Accuracy: 0.3666\n",
      "Epoch 41/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 4.3211e-04 - Accuracy: 0.3552 - val_loss: 5.1842e-04 - val_Accuracy: 0.3325\n",
      "Epoch 42/500\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 4.9394e-04 - Accuracy: 0.3616 - val_loss: 2.7049e-04 - val_Accuracy: 0.3719\n",
      "Epoch 43/500\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 4.7061e-04 - Accuracy: 0.3676 - val_loss: 3.4543e-04 - val_Accuracy: 0.3403\n",
      "Epoch 44/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 4.3303e-04 - Accuracy: 0.3680 - val_loss: 4.6255e-04 - val_Accuracy: 0.3653\n",
      "Epoch 45/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 4.4887e-04 - Accuracy: 0.3646 - val_loss: 3.4803e-04 - val_Accuracy: 0.3679\n",
      "Epoch 46/500\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 6.1135e-04 - Accuracy: 0.3643 - val_loss: 3.3320e-04 - val_Accuracy: 0.3548\n",
      "Epoch 47/500\n",
      "84/84 [==============================] - 3s 35ms/step - loss: 4.9475e-04 - Accuracy: 0.3643 - val_loss: 6.1446e-04 - val_Accuracy: 0.3771\n",
      "Epoch 48/500\n",
      "84/84 [==============================] - 3s 33ms/step - loss: 4.8499e-04 - Accuracy: 0.3624 - val_loss: 5.7111e-04 - val_Accuracy: 0.3548\n",
      "Epoch 49/500\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 4.9499e-04 - Accuracy: 0.3646 - val_loss: 3.5078e-04 - val_Accuracy: 0.3693\n",
      "Epoch 50/500\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 4.2701e-04 - Accuracy: 0.3684 - val_loss: 6.3604e-04 - val_Accuracy: 0.3443\n",
      "Epoch 51/500\n",
      "84/84 [==============================] - 2s 28ms/step - loss: 4.5435e-04 - Accuracy: 0.3684 - val_loss: 4.1731e-04 - val_Accuracy: 0.3522\n",
      "Epoch 52/500\n",
      "84/84 [==============================] - 2s 27ms/step - loss: 4.2099e-04 - Accuracy: 0.3665 - val_loss: 5.7385e-04 - val_Accuracy: 0.3417\n",
      "Epoch 53/500\n",
      "84/84 [==============================] - 2s 29ms/step - loss: 5.3951e-04 - Accuracy: 0.3710 - val_loss: 9.0372e-04 - val_Accuracy: 0.3219\n",
      "Epoch 54/500\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 5.3412e-04 - Accuracy: 0.3695 - val_loss: 5.9866e-04 - val_Accuracy: 0.3824\n",
      "Epoch 55/500\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 4.1993e-04 - Accuracy: 0.3699 - val_loss: 4.4760e-04 - val_Accuracy: 0.3653\n",
      "Epoch 56/500\n",
      "84/84 [==============================] - 2s 28ms/step - loss: 5.0509e-04 - Accuracy: 0.3680 - val_loss: 4.4994e-04 - val_Accuracy: 0.3561\n",
      "Epoch 57/500\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 3.9617e-04 - Accuracy: 0.3631 - val_loss: 4.5721e-04 - val_Accuracy: 0.3679\n",
      "Epoch 58/500\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 4.8829e-04 - Accuracy: 0.3620 - val_loss: 0.0014 - val_Accuracy: 0.3311\n",
      "Epoch 59/500\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 5.1838e-04 - Accuracy: 0.3620 - val_loss: 3.0013e-04 - val_Accuracy: 0.3758\n",
      "Epoch 60/500\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 4.1834e-04 - Accuracy: 0.3695 - val_loss: 4.1341e-04 - val_Accuracy: 0.3535\n",
      "Epoch 61/500\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 5.6097e-04 - Accuracy: 0.3665 - val_loss: 0.0013 - val_Accuracy: 0.3561\n",
      "Epoch 62/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 6.9945e-04 - Accuracy: 0.3582 - val_loss: 5.1901e-04 - val_Accuracy: 0.3771\n",
      "Epoch 63/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 5.1169e-04 - Accuracy: 0.3703 - val_loss: 5.1978e-04 - val_Accuracy: 0.3403\n",
      "Epoch 64/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 3.8998e-04 - Accuracy: 0.3680 - val_loss: 3.5136e-04 - val_Accuracy: 0.3614\n",
      "Epoch 65/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 4.5539e-04 - Accuracy: 0.3534 - val_loss: 3.3867e-04 - val_Accuracy: 0.3509\n",
      "Epoch 66/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 3.7314e-04 - Accuracy: 0.3624 - val_loss: 4.8007e-04 - val_Accuracy: 0.3732\n",
      "Epoch 67/500\n",
      "84/84 [==============================] - 1s 15ms/step - loss: 4.1167e-04 - Accuracy: 0.3714 - val_loss: 4.1993e-04 - val_Accuracy: 0.3535\n",
      "Epoch 68/500\n",
      "84/84 [==============================] - 1s 16ms/step - loss: 4.4522e-04 - Accuracy: 0.3620 - val_loss: 3.6648e-04 - val_Accuracy: 0.3601\n",
      "Epoch 69/500\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 3.4823e-04 - Accuracy: 0.3695 - val_loss: 4.3052e-04 - val_Accuracy: 0.3666\n",
      "Epoch 70/500\n",
      "84/84 [==============================] - 2s 18ms/step - loss: 3.7682e-04 - Accuracy: 0.3729 - val_loss: 6.6715e-04 - val_Accuracy: 0.3863\n",
      "Epoch 71/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 3.7764e-04 - Accuracy: 0.3643 - val_loss: 5.7984e-04 - val_Accuracy: 0.3771\n",
      "Epoch 72/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 3.9703e-04 - Accuracy: 0.3721 - val_loss: 6.4606e-04 - val_Accuracy: 0.3377\n",
      "Epoch 73/500\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 4.1163e-04 - Accuracy: 0.3684 - val_loss: 3.0126e-04 - val_Accuracy: 0.3771\n",
      "Epoch 74/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 3.9406e-04 - Accuracy: 0.3631 - val_loss: 5.3078e-04 - val_Accuracy: 0.3745\n",
      "Epoch 75/500\n",
      "84/84 [==============================] - 2s 27ms/step - loss: 3.2324e-04 - Accuracy: 0.3680 - val_loss: 8.3347e-04 - val_Accuracy: 0.3338\n",
      "Epoch 76/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 4.6537e-04 - Accuracy: 0.3586 - val_loss: 3.6405e-04 - val_Accuracy: 0.3601\n",
      "Epoch 77/500\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 4.4227e-04 - Accuracy: 0.3706 - val_loss: 3.2749e-04 - val_Accuracy: 0.3574\n",
      "Epoch 78/500\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 3.9362e-04 - Accuracy: 0.3654 - val_loss: 3.3569e-04 - val_Accuracy: 0.3574\n",
      "Epoch 79/500\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 3.8152e-04 - Accuracy: 0.3665 - val_loss: 5.4135e-04 - val_Accuracy: 0.3377\n",
      "Epoch 80/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 5.5672e-04 - Accuracy: 0.3616 - val_loss: 3.8695e-04 - val_Accuracy: 0.3509\n",
      "Epoch 81/500\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 3.7126e-04 - Accuracy: 0.3669 - val_loss: 4.8238e-04 - val_Accuracy: 0.3561\n",
      "Epoch 82/500\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 3.9670e-04 - Accuracy: 0.3736 - val_loss: 4.7967e-04 - val_Accuracy: 0.3482\n",
      "Epoch 83/500\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 3.9066e-04 - Accuracy: 0.3612 - val_loss: 4.3242e-04 - val_Accuracy: 0.3443\n",
      "Epoch 84/500\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 4.6566e-04 - Accuracy: 0.3567 - val_loss: 3.8123e-04 - val_Accuracy: 0.3417\n",
      "Epoch 85/500\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 4.2357e-04 - Accuracy: 0.3665 - val_loss: 4.3791e-04 - val_Accuracy: 0.3601\n",
      "Epoch 86/500\n",
      "84/84 [==============================] - 3s 30ms/step - loss: 3.7888e-04 - Accuracy: 0.3627 - val_loss: 3.3851e-04 - val_Accuracy: 0.3548\n",
      "Epoch 87/500\n",
      "84/84 [==============================] - 3s 31ms/step - loss: 3.6053e-04 - Accuracy: 0.3718 - val_loss: 2.7052e-04 - val_Accuracy: 0.3522\n",
      "Epoch 88/500\n",
      "84/84 [==============================] - 3s 34ms/step - loss: 3.5654e-04 - Accuracy: 0.3699 - val_loss: 3.5804e-04 - val_Accuracy: 0.3784\n",
      "Epoch 89/500\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 3.7290e-04 - Accuracy: 0.3691 - val_loss: 2.7100e-04 - val_Accuracy: 0.3850\n",
      "Epoch 90/500\n",
      "84/84 [==============================] - 2s 29ms/step - loss: 3.4624e-04 - Accuracy: 0.3736 - val_loss: 5.3824e-04 - val_Accuracy: 0.3693\n",
      "Epoch 91/500\n",
      "84/84 [==============================] - 3s 34ms/step - loss: 3.2667e-04 - Accuracy: 0.3695 - val_loss: 2.8619e-04 - val_Accuracy: 0.3548\n",
      "Epoch 92/500\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 3.9152e-04 - Accuracy: 0.3699 - val_loss: 5.2936e-04 - val_Accuracy: 0.3522\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_modelPred500.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "history1 = model.fit(X_train_scaled, y_train_scaled, validation_data=(X_val_scaled, y_val_scaled),\n",
    "                    epochs=500, callbacks=[early_stopping, model_checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "# best_modelA= tf.keras.models.load_model('best_modela.h5', custom_objects={'total_loss_function': total_loss_function})\n",
    "y_test_pred_scaled = model.predict(X_test_scaled)\n",
    "y_test_pred = scaler_y.inverse_transform(y_test_pred_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total Solids Flow_Concentrate</th>\n",
       "      <th>Total Solids Flow_Tailings</th>\n",
       "      <th>Cu_Tails</th>\n",
       "      <th>Fe_Tails</th>\n",
       "      <th>Pb_Tails</th>\n",
       "      <th>Zn_Tails</th>\n",
       "      <th>Cu_Concentrate</th>\n",
       "      <th>Fe_Concentrate</th>\n",
       "      <th>Pb_Concentrate</th>\n",
       "      <th>Zn_Concentrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>321.277794</td>\n",
       "      <td>91.913367</td>\n",
       "      <td>3.877782</td>\n",
       "      <td>19.841467</td>\n",
       "      <td>6.429478</td>\n",
       "      <td>9.794371</td>\n",
       "      <td>5.322058</td>\n",
       "      <td>19.459640</td>\n",
       "      <td>30.540651</td>\n",
       "      <td>6.333264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>341.367855</td>\n",
       "      <td>114.439065</td>\n",
       "      <td>4.063257</td>\n",
       "      <td>19.101297</td>\n",
       "      <td>7.137738</td>\n",
       "      <td>10.489607</td>\n",
       "      <td>4.669233</td>\n",
       "      <td>15.551373</td>\n",
       "      <td>37.530932</td>\n",
       "      <td>5.942357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>249.721830</td>\n",
       "      <td>92.119357</td>\n",
       "      <td>4.169189</td>\n",
       "      <td>19.031146</td>\n",
       "      <td>4.092684</td>\n",
       "      <td>9.728415</td>\n",
       "      <td>5.035712</td>\n",
       "      <td>17.538235</td>\n",
       "      <td>33.197211</td>\n",
       "      <td>6.143301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1684</th>\n",
       "      <td>61.907025</td>\n",
       "      <td>88.270661</td>\n",
       "      <td>4.731239</td>\n",
       "      <td>22.264424</td>\n",
       "      <td>2.738228</td>\n",
       "      <td>6.879406</td>\n",
       "      <td>5.838119</td>\n",
       "      <td>20.678163</td>\n",
       "      <td>22.400555</td>\n",
       "      <td>5.252855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1653</th>\n",
       "      <td>58.968723</td>\n",
       "      <td>81.021827</td>\n",
       "      <td>3.422589</td>\n",
       "      <td>21.064611</td>\n",
       "      <td>2.941296</td>\n",
       "      <td>10.923154</td>\n",
       "      <td>4.273590</td>\n",
       "      <td>19.928913</td>\n",
       "      <td>23.317810</td>\n",
       "      <td>7.938129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1080</th>\n",
       "      <td>330.514152</td>\n",
       "      <td>82.046666</td>\n",
       "      <td>5.033038</td>\n",
       "      <td>22.135906</td>\n",
       "      <td>4.500763</td>\n",
       "      <td>7.135248</td>\n",
       "      <td>5.044595</td>\n",
       "      <td>18.492300</td>\n",
       "      <td>35.620484</td>\n",
       "      <td>4.935209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2564</th>\n",
       "      <td>69.301904</td>\n",
       "      <td>85.341182</td>\n",
       "      <td>4.370904</td>\n",
       "      <td>19.348762</td>\n",
       "      <td>3.408323</td>\n",
       "      <td>8.313208</td>\n",
       "      <td>5.168326</td>\n",
       "      <td>17.556302</td>\n",
       "      <td>24.773259</td>\n",
       "      <td>6.204238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>332.851039</td>\n",
       "      <td>98.001376</td>\n",
       "      <td>4.318070</td>\n",
       "      <td>20.112517</td>\n",
       "      <td>3.902605</td>\n",
       "      <td>11.477110</td>\n",
       "      <td>5.833442</td>\n",
       "      <td>19.345911</td>\n",
       "      <td>28.951970</td>\n",
       "      <td>7.348339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2208</th>\n",
       "      <td>345.474955</td>\n",
       "      <td>90.497394</td>\n",
       "      <td>5.052613</td>\n",
       "      <td>15.493180</td>\n",
       "      <td>4.571156</td>\n",
       "      <td>12.656511</td>\n",
       "      <td>6.092747</td>\n",
       "      <td>16.267235</td>\n",
       "      <td>34.204651</td>\n",
       "      <td>6.812318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2958</th>\n",
       "      <td>64.938328</td>\n",
       "      <td>71.340121</td>\n",
       "      <td>5.256093</td>\n",
       "      <td>18.410262</td>\n",
       "      <td>3.661872</td>\n",
       "      <td>9.973627</td>\n",
       "      <td>6.151233</td>\n",
       "      <td>17.201252</td>\n",
       "      <td>22.359305</td>\n",
       "      <td>7.606174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>381 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Total Solids Flow_Concentrate  Total Solids Flow_Tailings  Cu_Tails  \\\n",
       "2014                     321.277794                   91.913367  3.877782   \n",
       "527                      341.367855                  114.439065  4.063257   \n",
       "478                      249.721830                   92.119357  4.169189   \n",
       "1684                      61.907025                   88.270661  4.731239   \n",
       "1653                      58.968723                   81.021827  3.422589   \n",
       "...                             ...                         ...       ...   \n",
       "1080                     330.514152                   82.046666  5.033038   \n",
       "2564                      69.301904                   85.341182  4.370904   \n",
       "862                      332.851039                   98.001376  4.318070   \n",
       "2208                     345.474955                   90.497394  5.052613   \n",
       "2958                      64.938328                   71.340121  5.256093   \n",
       "\n",
       "       Fe_Tails  Pb_Tails   Zn_Tails  Cu_Concentrate  Fe_Concentrate  \\\n",
       "2014  19.841467  6.429478   9.794371        5.322058       19.459640   \n",
       "527   19.101297  7.137738  10.489607        4.669233       15.551373   \n",
       "478   19.031146  4.092684   9.728415        5.035712       17.538235   \n",
       "1684  22.264424  2.738228   6.879406        5.838119       20.678163   \n",
       "1653  21.064611  2.941296  10.923154        4.273590       19.928913   \n",
       "...         ...       ...        ...             ...             ...   \n",
       "1080  22.135906  4.500763   7.135248        5.044595       18.492300   \n",
       "2564  19.348762  3.408323   8.313208        5.168326       17.556302   \n",
       "862   20.112517  3.902605  11.477110        5.833442       19.345911   \n",
       "2208  15.493180  4.571156  12.656511        6.092747       16.267235   \n",
       "2958  18.410262  3.661872   9.973627        6.151233       17.201252   \n",
       "\n",
       "      Pb_Concentrate  Zn_Concentrate  \n",
       "2014       30.540651        6.333264  \n",
       "527        37.530932        5.942357  \n",
       "478        33.197211        6.143301  \n",
       "1684       22.400555        5.252855  \n",
       "1653       23.317810        7.938129  \n",
       "...              ...             ...  \n",
       "1080       35.620484        4.935209  \n",
       "2564       24.773259        6.204238  \n",
       "862        28.951970        7.348339  \n",
       "2208       34.204651        6.812318  \n",
       "2958       22.359305        7.606174  \n",
       "\n",
       "[381 rows x 10 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Net Volume</th>\n",
       "      <th>Pulp Area</th>\n",
       "      <th>Froth surface area</th>\n",
       "      <th>Froth thickness</th>\n",
       "      <th>Air Flow rate</th>\n",
       "      <th>R_inf Ccp</th>\n",
       "      <th>R_inf Gn</th>\n",
       "      <th>R_inf Po</th>\n",
       "      <th>R_inf Sp</th>\n",
       "      <th>k_max Ccp</th>\n",
       "      <th>...</th>\n",
       "      <th>Total Solids Flow_Feed</th>\n",
       "      <th>Total Liquid Flow_Feed</th>\n",
       "      <th>Pulp Volumetric Flow_Feed</th>\n",
       "      <th>Solids SG_Feed</th>\n",
       "      <th>Pulp SG_Feed</th>\n",
       "      <th>Solids Fraction_Feed</th>\n",
       "      <th>Cu_Feed</th>\n",
       "      <th>Fe_Feed</th>\n",
       "      <th>Pb_Feed</th>\n",
       "      <th>Zn_Feed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>60.295910</td>\n",
       "      <td>41.752645</td>\n",
       "      <td>37.577381</td>\n",
       "      <td>20.850229</td>\n",
       "      <td>9.979310</td>\n",
       "      <td>30.180197</td>\n",
       "      <td>78.697154</td>\n",
       "      <td>19.679781</td>\n",
       "      <td>15.661964</td>\n",
       "      <td>1.694448</td>\n",
       "      <td>...</td>\n",
       "      <td>413.191161</td>\n",
       "      <td>655.605715</td>\n",
       "      <td>744.145499</td>\n",
       "      <td>4.772750</td>\n",
       "      <td>1.436274</td>\n",
       "      <td>11.633876</td>\n",
       "      <td>5.000782</td>\n",
       "      <td>19.544576</td>\n",
       "      <td>25.177180</td>\n",
       "      <td>7.103179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>39.579255</td>\n",
       "      <td>39.679225</td>\n",
       "      <td>35.711303</td>\n",
       "      <td>13.340962</td>\n",
       "      <td>8.527548</td>\n",
       "      <td>29.463456</td>\n",
       "      <td>79.758318</td>\n",
       "      <td>19.483449</td>\n",
       "      <td>16.055934</td>\n",
       "      <td>2.384209</td>\n",
       "      <td>...</td>\n",
       "      <td>455.806919</td>\n",
       "      <td>660.114204</td>\n",
       "      <td>755.926672</td>\n",
       "      <td>4.857685</td>\n",
       "      <td>1.476229</td>\n",
       "      <td>12.412861</td>\n",
       "      <td>4.517091</td>\n",
       "      <td>16.442649</td>\n",
       "      <td>29.900138</td>\n",
       "      <td>7.084031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>74.823320</td>\n",
       "      <td>32.876450</td>\n",
       "      <td>29.588805</td>\n",
       "      <td>16.165099</td>\n",
       "      <td>9.267705</td>\n",
       "      <td>30.479647</td>\n",
       "      <td>79.166093</td>\n",
       "      <td>19.958770</td>\n",
       "      <td>14.880530</td>\n",
       "      <td>1.427834</td>\n",
       "      <td>...</td>\n",
       "      <td>341.841187</td>\n",
       "      <td>476.428833</td>\n",
       "      <td>550.830008</td>\n",
       "      <td>4.684560</td>\n",
       "      <td>1.485522</td>\n",
       "      <td>13.247624</td>\n",
       "      <td>4.802202</td>\n",
       "      <td>17.940545</td>\n",
       "      <td>25.354124</td>\n",
       "      <td>7.109418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1684</th>\n",
       "      <td>56.247185</td>\n",
       "      <td>31.136481</td>\n",
       "      <td>28.022833</td>\n",
       "      <td>19.219430</td>\n",
       "      <td>9.163836</td>\n",
       "      <td>29.524031</td>\n",
       "      <td>80.670036</td>\n",
       "      <td>18.525955</td>\n",
       "      <td>15.209618</td>\n",
       "      <td>1.461505</td>\n",
       "      <td>...</td>\n",
       "      <td>150.177686</td>\n",
       "      <td>163.996063</td>\n",
       "      <td>200.931881</td>\n",
       "      <td>4.120799</td>\n",
       "      <td>1.563583</td>\n",
       "      <td>18.137406</td>\n",
       "      <td>5.187523</td>\n",
       "      <td>21.610527</td>\n",
       "      <td>10.843535</td>\n",
       "      <td>6.208901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1653</th>\n",
       "      <td>68.548227</td>\n",
       "      <td>35.887352</td>\n",
       "      <td>32.298616</td>\n",
       "      <td>22.096892</td>\n",
       "      <td>11.116597</td>\n",
       "      <td>30.366229</td>\n",
       "      <td>79.755254</td>\n",
       "      <td>21.068642</td>\n",
       "      <td>14.176491</td>\n",
       "      <td>1.613017</td>\n",
       "      <td>...</td>\n",
       "      <td>139.990550</td>\n",
       "      <td>154.608171</td>\n",
       "      <td>188.786475</td>\n",
       "      <td>4.152238</td>\n",
       "      <td>1.560486</td>\n",
       "      <td>17.858525</td>\n",
       "      <td>3.781059</td>\n",
       "      <td>20.586217</td>\n",
       "      <td>11.524569</td>\n",
       "      <td>9.665761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1080</th>\n",
       "      <td>74.625145</td>\n",
       "      <td>34.712758</td>\n",
       "      <td>31.241482</td>\n",
       "      <td>21.320908</td>\n",
       "      <td>11.425787</td>\n",
       "      <td>27.674468</td>\n",
       "      <td>79.638842</td>\n",
       "      <td>21.332461</td>\n",
       "      <td>14.933820</td>\n",
       "      <td>1.988014</td>\n",
       "      <td>...</td>\n",
       "      <td>412.560818</td>\n",
       "      <td>683.391275</td>\n",
       "      <td>769.402993</td>\n",
       "      <td>4.913688</td>\n",
       "      <td>1.424419</td>\n",
       "      <td>10.912558</td>\n",
       "      <td>5.042297</td>\n",
       "      <td>19.216910</td>\n",
       "      <td>29.431652</td>\n",
       "      <td>5.372734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2564</th>\n",
       "      <td>63.091520</td>\n",
       "      <td>38.101469</td>\n",
       "      <td>34.291322</td>\n",
       "      <td>15.095530</td>\n",
       "      <td>9.816964</td>\n",
       "      <td>31.463651</td>\n",
       "      <td>81.591301</td>\n",
       "      <td>19.708939</td>\n",
       "      <td>15.182544</td>\n",
       "      <td>0.987981</td>\n",
       "      <td>...</td>\n",
       "      <td>154.643086</td>\n",
       "      <td>156.706428</td>\n",
       "      <td>194.683674</td>\n",
       "      <td>4.123032</td>\n",
       "      <td>1.599258</td>\n",
       "      <td>19.265677</td>\n",
       "      <td>4.728262</td>\n",
       "      <td>18.545487</td>\n",
       "      <td>12.982826</td>\n",
       "      <td>7.368092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>59.466317</td>\n",
       "      <td>36.358165</td>\n",
       "      <td>32.722349</td>\n",
       "      <td>21.706842</td>\n",
       "      <td>10.560952</td>\n",
       "      <td>29.808806</td>\n",
       "      <td>78.740827</td>\n",
       "      <td>19.368714</td>\n",
       "      <td>15.471187</td>\n",
       "      <td>2.412993</td>\n",
       "      <td>...</td>\n",
       "      <td>430.852416</td>\n",
       "      <td>651.834886</td>\n",
       "      <td>745.189502</td>\n",
       "      <td>4.713967</td>\n",
       "      <td>1.452902</td>\n",
       "      <td>12.265217</td>\n",
       "      <td>5.488756</td>\n",
       "      <td>19.520283</td>\n",
       "      <td>23.254260</td>\n",
       "      <td>8.287466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2208</th>\n",
       "      <td>81.537113</td>\n",
       "      <td>33.741592</td>\n",
       "      <td>30.367433</td>\n",
       "      <td>30.485407</td>\n",
       "      <td>12.468185</td>\n",
       "      <td>30.526359</td>\n",
       "      <td>81.657019</td>\n",
       "      <td>19.368081</td>\n",
       "      <td>14.655859</td>\n",
       "      <td>2.142512</td>\n",
       "      <td>...</td>\n",
       "      <td>435.972349</td>\n",
       "      <td>691.690628</td>\n",
       "      <td>784.309697</td>\n",
       "      <td>4.815033</td>\n",
       "      <td>1.437778</td>\n",
       "      <td>11.544419</td>\n",
       "      <td>5.876840</td>\n",
       "      <td>16.106560</td>\n",
       "      <td>28.053449</td>\n",
       "      <td>8.025432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2958</th>\n",
       "      <td>58.468169</td>\n",
       "      <td>42.866070</td>\n",
       "      <td>38.579463</td>\n",
       "      <td>16.296654</td>\n",
       "      <td>9.193254</td>\n",
       "      <td>30.453449</td>\n",
       "      <td>79.080952</td>\n",
       "      <td>20.262725</td>\n",
       "      <td>15.052476</td>\n",
       "      <td>1.957637</td>\n",
       "      <td>...</td>\n",
       "      <td>136.278449</td>\n",
       "      <td>127.787368</td>\n",
       "      <td>161.170511</td>\n",
       "      <td>4.129677</td>\n",
       "      <td>1.638425</td>\n",
       "      <td>20.475073</td>\n",
       "      <td>5.682638</td>\n",
       "      <td>17.834154</td>\n",
       "      <td>12.571424</td>\n",
       "      <td>8.845507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>381 rows  24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Net Volume  Pulp Area  Froth surface area  Froth thickness  \\\n",
       "2014   60.295910  41.752645           37.577381        20.850229   \n",
       "527    39.579255  39.679225           35.711303        13.340962   \n",
       "478    74.823320  32.876450           29.588805        16.165099   \n",
       "1684   56.247185  31.136481           28.022833        19.219430   \n",
       "1653   68.548227  35.887352           32.298616        22.096892   \n",
       "...          ...        ...                 ...              ...   \n",
       "1080   74.625145  34.712758           31.241482        21.320908   \n",
       "2564   63.091520  38.101469           34.291322        15.095530   \n",
       "862    59.466317  36.358165           32.722349        21.706842   \n",
       "2208   81.537113  33.741592           30.367433        30.485407   \n",
       "2958   58.468169  42.866070           38.579463        16.296654   \n",
       "\n",
       "      Air Flow rate  R_inf Ccp   R_inf Gn   R_inf Po   R_inf Sp  k_max Ccp  \\\n",
       "2014       9.979310  30.180197  78.697154  19.679781  15.661964   1.694448   \n",
       "527        8.527548  29.463456  79.758318  19.483449  16.055934   2.384209   \n",
       "478        9.267705  30.479647  79.166093  19.958770  14.880530   1.427834   \n",
       "1684       9.163836  29.524031  80.670036  18.525955  15.209618   1.461505   \n",
       "1653      11.116597  30.366229  79.755254  21.068642  14.176491   1.613017   \n",
       "...             ...        ...        ...        ...        ...        ...   \n",
       "1080      11.425787  27.674468  79.638842  21.332461  14.933820   1.988014   \n",
       "2564       9.816964  31.463651  81.591301  19.708939  15.182544   0.987981   \n",
       "862       10.560952  29.808806  78.740827  19.368714  15.471187   2.412993   \n",
       "2208      12.468185  30.526359  81.657019  19.368081  14.655859   2.142512   \n",
       "2958       9.193254  30.453449  79.080952  20.262725  15.052476   1.957637   \n",
       "\n",
       "      ...  Total Solids Flow_Feed  Total Liquid Flow_Feed  \\\n",
       "2014  ...              413.191161              655.605715   \n",
       "527   ...              455.806919              660.114204   \n",
       "478   ...              341.841187              476.428833   \n",
       "1684  ...              150.177686              163.996063   \n",
       "1653  ...              139.990550              154.608171   \n",
       "...   ...                     ...                     ...   \n",
       "1080  ...              412.560818              683.391275   \n",
       "2564  ...              154.643086              156.706428   \n",
       "862   ...              430.852416              651.834886   \n",
       "2208  ...              435.972349              691.690628   \n",
       "2958  ...              136.278449              127.787368   \n",
       "\n",
       "      Pulp Volumetric Flow_Feed  Solids SG_Feed  Pulp SG_Feed  \\\n",
       "2014                 744.145499        4.772750      1.436274   \n",
       "527                  755.926672        4.857685      1.476229   \n",
       "478                  550.830008        4.684560      1.485522   \n",
       "1684                 200.931881        4.120799      1.563583   \n",
       "1653                 188.786475        4.152238      1.560486   \n",
       "...                         ...             ...           ...   \n",
       "1080                 769.402993        4.913688      1.424419   \n",
       "2564                 194.683674        4.123032      1.599258   \n",
       "862                  745.189502        4.713967      1.452902   \n",
       "2208                 784.309697        4.815033      1.437778   \n",
       "2958                 161.170511        4.129677      1.638425   \n",
       "\n",
       "      Solids Fraction_Feed   Cu_Feed    Fe_Feed    Pb_Feed   Zn_Feed  \n",
       "2014             11.633876  5.000782  19.544576  25.177180  7.103179  \n",
       "527              12.412861  4.517091  16.442649  29.900138  7.084031  \n",
       "478              13.247624  4.802202  17.940545  25.354124  7.109418  \n",
       "1684             18.137406  5.187523  21.610527  10.843535  6.208901  \n",
       "1653             17.858525  3.781059  20.586217  11.524569  9.665761  \n",
       "...                    ...       ...        ...        ...       ...  \n",
       "1080             10.912558  5.042297  19.216910  29.431652  5.372734  \n",
       "2564             19.265677  4.728262  18.545487  12.982826  7.368092  \n",
       "862              12.265217  5.488756  19.520283  23.254260  8.287466  \n",
       "2208             11.544419  5.876840  16.106560  28.053449  8.025432  \n",
       "2958             20.475073  5.682638  17.834154  12.571424  8.845507  \n",
       "\n",
       "[381 rows x 24 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total Solids Flow_Concentrate</th>\n",
       "      <th>Total Solids Flow_Tailings</th>\n",
       "      <th>Cu_Tails</th>\n",
       "      <th>Fe_Tails</th>\n",
       "      <th>Pb_Tails</th>\n",
       "      <th>Zn_Tails</th>\n",
       "      <th>Cu_Concentrate</th>\n",
       "      <th>Fe_Concentrate</th>\n",
       "      <th>Pb_Concentrate</th>\n",
       "      <th>Zn_Concentrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>322.580536</td>\n",
       "      <td>92.384499</td>\n",
       "      <td>4.318418</td>\n",
       "      <td>18.584591</td>\n",
       "      <td>4.795500</td>\n",
       "      <td>10.252101</td>\n",
       "      <td>5.254008</td>\n",
       "      <td>17.446392</td>\n",
       "      <td>32.370258</td>\n",
       "      <td>5.762011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>347.281555</td>\n",
       "      <td>112.825684</td>\n",
       "      <td>4.226645</td>\n",
       "      <td>19.065271</td>\n",
       "      <td>5.194051</td>\n",
       "      <td>10.334233</td>\n",
       "      <td>5.253893</td>\n",
       "      <td>17.088778</td>\n",
       "      <td>33.117786</td>\n",
       "      <td>6.331732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>285.604828</td>\n",
       "      <td>81.980644</td>\n",
       "      <td>4.122493</td>\n",
       "      <td>18.208979</td>\n",
       "      <td>4.593539</td>\n",
       "      <td>9.770304</td>\n",
       "      <td>5.171039</td>\n",
       "      <td>17.383211</td>\n",
       "      <td>31.108595</td>\n",
       "      <td>5.549874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63.282974</td>\n",
       "      <td>89.801788</td>\n",
       "      <td>3.855698</td>\n",
       "      <td>18.746412</td>\n",
       "      <td>3.395491</td>\n",
       "      <td>9.491824</td>\n",
       "      <td>4.915816</td>\n",
       "      <td>14.557428</td>\n",
       "      <td>24.833967</td>\n",
       "      <td>6.705284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56.690624</td>\n",
       "      <td>83.778999</td>\n",
       "      <td>3.852165</td>\n",
       "      <td>18.073742</td>\n",
       "      <td>2.970007</td>\n",
       "      <td>9.324373</td>\n",
       "      <td>4.691880</td>\n",
       "      <td>14.580379</td>\n",
       "      <td>23.117743</td>\n",
       "      <td>6.183370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>327.696716</td>\n",
       "      <td>81.748322</td>\n",
       "      <td>3.913805</td>\n",
       "      <td>19.463623</td>\n",
       "      <td>5.432650</td>\n",
       "      <td>9.519204</td>\n",
       "      <td>5.437150</td>\n",
       "      <td>16.802694</td>\n",
       "      <td>35.131428</td>\n",
       "      <td>6.028932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>72.857483</td>\n",
       "      <td>84.186287</td>\n",
       "      <td>4.010791</td>\n",
       "      <td>17.839058</td>\n",
       "      <td>2.773958</td>\n",
       "      <td>9.544034</td>\n",
       "      <td>4.609912</td>\n",
       "      <td>14.865541</td>\n",
       "      <td>22.525723</td>\n",
       "      <td>5.958226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>333.751831</td>\n",
       "      <td>99.269646</td>\n",
       "      <td>4.496642</td>\n",
       "      <td>18.427340</td>\n",
       "      <td>4.663771</td>\n",
       "      <td>10.632979</td>\n",
       "      <td>5.224047</td>\n",
       "      <td>17.751297</td>\n",
       "      <td>31.866995</td>\n",
       "      <td>5.739261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>342.332428</td>\n",
       "      <td>89.696457</td>\n",
       "      <td>4.104436</td>\n",
       "      <td>19.396208</td>\n",
       "      <td>5.371769</td>\n",
       "      <td>9.928329</td>\n",
       "      <td>5.429724</td>\n",
       "      <td>17.015779</td>\n",
       "      <td>34.973793</td>\n",
       "      <td>6.066076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>62.684849</td>\n",
       "      <td>75.118797</td>\n",
       "      <td>3.555194</td>\n",
       "      <td>17.094391</td>\n",
       "      <td>2.781208</td>\n",
       "      <td>8.505760</td>\n",
       "      <td>4.412202</td>\n",
       "      <td>14.189796</td>\n",
       "      <td>21.708799</td>\n",
       "      <td>5.495682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>381 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Total Solids Flow_Concentrate  Total Solids Flow_Tailings  Cu_Tails  \\\n",
       "0                       322.580536                   92.384499  4.318418   \n",
       "1                       347.281555                  112.825684  4.226645   \n",
       "2                       285.604828                   81.980644  4.122493   \n",
       "3                        63.282974                   89.801788  3.855698   \n",
       "4                        56.690624                   83.778999  3.852165   \n",
       "..                             ...                         ...       ...   \n",
       "376                     327.696716                   81.748322  3.913805   \n",
       "377                      72.857483                   84.186287  4.010791   \n",
       "378                     333.751831                   99.269646  4.496642   \n",
       "379                     342.332428                   89.696457  4.104436   \n",
       "380                      62.684849                   75.118797  3.555194   \n",
       "\n",
       "      Fe_Tails  Pb_Tails   Zn_Tails  Cu_Concentrate  Fe_Concentrate  \\\n",
       "0    18.584591  4.795500  10.252101        5.254008       17.446392   \n",
       "1    19.065271  5.194051  10.334233        5.253893       17.088778   \n",
       "2    18.208979  4.593539   9.770304        5.171039       17.383211   \n",
       "3    18.746412  3.395491   9.491824        4.915816       14.557428   \n",
       "4    18.073742  2.970007   9.324373        4.691880       14.580379   \n",
       "..         ...       ...        ...             ...             ...   \n",
       "376  19.463623  5.432650   9.519204        5.437150       16.802694   \n",
       "377  17.839058  2.773958   9.544034        4.609912       14.865541   \n",
       "378  18.427340  4.663771  10.632979        5.224047       17.751297   \n",
       "379  19.396208  5.371769   9.928329        5.429724       17.015779   \n",
       "380  17.094391  2.781208   8.505760        4.412202       14.189796   \n",
       "\n",
       "     Pb_Concentrate  Zn_Concentrate  \n",
       "0         32.370258        5.762011  \n",
       "1         33.117786        6.331732  \n",
       "2         31.108595        5.549874  \n",
       "3         24.833967        6.705284  \n",
       "4         23.117743        6.183370  \n",
       "..              ...             ...  \n",
       "376       35.131428        6.028932  \n",
       "377       22.525723        5.958226  \n",
       "378       31.866995        5.739261  \n",
       "379       34.973793        6.066076  \n",
       "380       21.708799        5.495682  \n",
       "\n",
       "[381 rows x 10 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_test_scaled_df = pd.DataFrame(y_test_pred, columns=output_columns)\n",
    "y_test_scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maghr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('PINN500EP.h5')  # Saves the model in HDF5 file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model(r'C:\\Users\\maghr\\OneDrive\\Documents\\Big Data\\projets\\PINN_notebook\\PINN500EP.h5', custom_objects={'total_loss_function': total_loss_function})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 1s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_train_scaled)\n",
    "y_test_pred = scaler_y.inverse_transform(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total Solids Flow_Concentrate</th>\n",
       "      <th>Total Solids Flow_Tailings</th>\n",
       "      <th>Cu_Tails</th>\n",
       "      <th>Fe_Tails</th>\n",
       "      <th>Pb_Tails</th>\n",
       "      <th>Zn_Tails</th>\n",
       "      <th>Cu_Concentrate</th>\n",
       "      <th>Fe_Concentrate</th>\n",
       "      <th>Pb_Concentrate</th>\n",
       "      <th>Zn_Concentrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>329.287598</td>\n",
       "      <td>85.838409</td>\n",
       "      <td>4.401943</td>\n",
       "      <td>19.221107</td>\n",
       "      <td>4.959444</td>\n",
       "      <td>10.037381</td>\n",
       "      <td>5.487308</td>\n",
       "      <td>17.293421</td>\n",
       "      <td>34.284325</td>\n",
       "      <td>6.084352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.612427</td>\n",
       "      <td>87.105026</td>\n",
       "      <td>4.308586</td>\n",
       "      <td>19.210268</td>\n",
       "      <td>2.704384</td>\n",
       "      <td>10.345457</td>\n",
       "      <td>5.092942</td>\n",
       "      <td>17.641790</td>\n",
       "      <td>23.467897</td>\n",
       "      <td>7.624357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>321.344238</td>\n",
       "      <td>81.906631</td>\n",
       "      <td>4.416780</td>\n",
       "      <td>19.080679</td>\n",
       "      <td>4.991702</td>\n",
       "      <td>9.994049</td>\n",
       "      <td>5.501955</td>\n",
       "      <td>17.269312</td>\n",
       "      <td>34.286610</td>\n",
       "      <td>6.066712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61.169369</td>\n",
       "      <td>89.060089</td>\n",
       "      <td>4.296291</td>\n",
       "      <td>19.281721</td>\n",
       "      <td>2.609103</td>\n",
       "      <td>10.377015</td>\n",
       "      <td>5.071957</td>\n",
       "      <td>17.694941</td>\n",
       "      <td>22.906851</td>\n",
       "      <td>7.711910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>328.791992</td>\n",
       "      <td>95.403915</td>\n",
       "      <td>4.341011</td>\n",
       "      <td>19.664474</td>\n",
       "      <td>5.376546</td>\n",
       "      <td>10.068645</td>\n",
       "      <td>5.539026</td>\n",
       "      <td>17.999084</td>\n",
       "      <td>32.252350</td>\n",
       "      <td>6.398694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2658</th>\n",
       "      <td>347.113586</td>\n",
       "      <td>90.352463</td>\n",
       "      <td>4.395817</td>\n",
       "      <td>19.337656</td>\n",
       "      <td>4.704726</td>\n",
       "      <td>10.119705</td>\n",
       "      <td>5.432124</td>\n",
       "      <td>17.036903</td>\n",
       "      <td>35.171799</td>\n",
       "      <td>5.985380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2659</th>\n",
       "      <td>343.041656</td>\n",
       "      <td>101.303535</td>\n",
       "      <td>4.321668</td>\n",
       "      <td>19.863228</td>\n",
       "      <td>5.269869</td>\n",
       "      <td>10.142384</td>\n",
       "      <td>5.506777</td>\n",
       "      <td>17.959179</td>\n",
       "      <td>32.487206</td>\n",
       "      <td>6.393246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2660</th>\n",
       "      <td>65.442863</td>\n",
       "      <td>81.266380</td>\n",
       "      <td>4.426708</td>\n",
       "      <td>18.544739</td>\n",
       "      <td>3.117697</td>\n",
       "      <td>10.326519</td>\n",
       "      <td>5.097972</td>\n",
       "      <td>16.926407</td>\n",
       "      <td>24.184650</td>\n",
       "      <td>7.483603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2661</th>\n",
       "      <td>78.309425</td>\n",
       "      <td>92.759216</td>\n",
       "      <td>4.277956</td>\n",
       "      <td>19.460541</td>\n",
       "      <td>2.941715</td>\n",
       "      <td>10.366340</td>\n",
       "      <td>5.121741</td>\n",
       "      <td>17.966759</td>\n",
       "      <td>22.794174</td>\n",
       "      <td>7.737429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2662</th>\n",
       "      <td>329.859955</td>\n",
       "      <td>95.694847</td>\n",
       "      <td>4.340515</td>\n",
       "      <td>19.672401</td>\n",
       "      <td>5.362153</td>\n",
       "      <td>10.073647</td>\n",
       "      <td>5.535826</td>\n",
       "      <td>17.985193</td>\n",
       "      <td>32.301273</td>\n",
       "      <td>6.393424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2663 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Total Solids Flow_Concentrate  Total Solids Flow_Tailings  Cu_Tails  \\\n",
       "0                        329.287598                   85.838409  4.401943   \n",
       "1                         69.612427                   87.105026  4.308586   \n",
       "2                        321.344238                   81.906631  4.416780   \n",
       "3                         61.169369                   89.060089  4.296291   \n",
       "4                        328.791992                   95.403915  4.341011   \n",
       "...                             ...                         ...       ...   \n",
       "2658                     347.113586                   90.352463  4.395817   \n",
       "2659                     343.041656                  101.303535  4.321668   \n",
       "2660                      65.442863                   81.266380  4.426708   \n",
       "2661                      78.309425                   92.759216  4.277956   \n",
       "2662                     329.859955                   95.694847  4.340515   \n",
       "\n",
       "       Fe_Tails  Pb_Tails   Zn_Tails  Cu_Concentrate  Fe_Concentrate  \\\n",
       "0     19.221107  4.959444  10.037381        5.487308       17.293421   \n",
       "1     19.210268  2.704384  10.345457        5.092942       17.641790   \n",
       "2     19.080679  4.991702   9.994049        5.501955       17.269312   \n",
       "3     19.281721  2.609103  10.377015        5.071957       17.694941   \n",
       "4     19.664474  5.376546  10.068645        5.539026       17.999084   \n",
       "...         ...       ...        ...             ...             ...   \n",
       "2658  19.337656  4.704726  10.119705        5.432124       17.036903   \n",
       "2659  19.863228  5.269869  10.142384        5.506777       17.959179   \n",
       "2660  18.544739  3.117697  10.326519        5.097972       16.926407   \n",
       "2661  19.460541  2.941715  10.366340        5.121741       17.966759   \n",
       "2662  19.672401  5.362153  10.073647        5.535826       17.985193   \n",
       "\n",
       "      Pb_Concentrate  Zn_Concentrate  \n",
       "0          34.284325        6.084352  \n",
       "1          23.467897        7.624357  \n",
       "2          34.286610        6.066712  \n",
       "3          22.906851        7.711910  \n",
       "4          32.252350        6.398694  \n",
       "...              ...             ...  \n",
       "2658       35.171799        5.985380  \n",
       "2659       32.487206        6.393246  \n",
       "2660       24.184650        7.483603  \n",
       "2661       22.794174        7.737429  \n",
       "2662       32.301273        6.393424  \n",
       "\n",
       "[2663 rows x 10 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming y_pred is your predictions array and output_columns are your column names\n",
    "predictions_df = pd.DataFrame(y_test_pred, columns=output_columns)\n",
    "\n",
    "# Now you can use the to_excel method\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions_dfff' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpredictions_dfff\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction_test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predictions_dfff' is not defined"
     ]
    }
   ],
   "source": [
    "predictions_dfff.to_csv('prediction_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extracting loss values from the history object\n",
    "training_loss = historyT.history['loss']\n",
    "validation_loss = historyT.history['val_loss']\n",
    "epochs = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs, training_loss, 'bo-', label='Training Loss')\n",
    "# plt.plot(epochs, validation_loss, 'ro-', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(historyT.history['loss'], label='Training Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.1.2-cp310-cp310-win_amd64.whl (192.3 MB)\n",
      "     -------------------                    100.6/192.3 MB 4.0 MB/s eta 0:00:23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\maghr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 435, in _error_catcher\n",
      "    yield\n",
      "  File \"c:\\Users\\maghr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 516, in read\n",
      "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
      "  File \"c:\\Users\\maghr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 94, in read\n",
      "    self.__buf.write(data)\n",
      "  File \"c:\\Users\\maghr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\tempfile.py\", line 483, in func_wrapper\n",
      "    return func(*args, **kwargs)\n",
      "OSError: [Errno 28] No space left on device\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\maghr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 167, in exc_logging_wrapper\n",
      "    status = run_func(*args)\n",
      "  File \"c:\\Users\\maghr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 247, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"c:\\Users\\maghr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 369, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "  File \"c:\\Users\\maghr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 92, in resolve\n",
      "    result = self._result = resolver.resolve(\n",
      "  File \"c:\\Users\\maghr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 481, in resolve\n",
      "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
      "  File \"c:\\Users\\maghr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 348, in resolve\n",
      "    self._add_to_criteria(self.state.criteria, r, parent=None)\n",
      "  File \"c:\\Users\\maghr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 172, in _add_to_criteria\n",
      "    if not criterion.candidates:\n",
      "  File \"c:\\Users\\maghr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_vendor\\resolvelib\\structs.py\", line 151, in __bool__\n",
      "    return bool(self._sequence)\n",
      "  File \"c:\\Users\\maghr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 155, in __bool__\n",
      "    return any(self)\n",
      "  File \"c:\\Users\\maghr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 143, in <genexpr>\n",
      "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
      "  File \"c:\\Users\\maghr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 47, in _iter_built\n",
      "    candidate = func()\n",
      "  File \"c:\\Users\\maghr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 206, in _make_candidate_from_link\n",
      "    self._link_candidate_cache[link] = LinkCandidate(\n",
      "  File \"c:\\Users\\maghr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 297, in __init__\n",
      "    super().__init__(\n",
      "  File \"c:\\Users\\maghr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 162, in __init__\n",
      "    self.dist = self._prepare()\n",
      "  File \"c:\\Users\\maghr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 231, in _prepare\n",
      "    dist = self._prepare_distribution()\n",
      "  File \"c:\\Users\\maghr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 308, in _prepare_distribution\n",
      "    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n",
      "  File \"c:\\Users\\maghr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 438, in prepare_linked_requirement\n",
      "    return self._prepare_linked_requirement(req, parallel_builds)\n",
      "  File \"c:\\Users\\maghr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 483, in _prepare_linked_requirement\n",
      "    local_file = unpack_url(\n",
      "  File \"c:\\Users\\maghr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 165, in unpack_url\n",
      "    file = get_http_url(\n",
      "  File \"c:\\Users\\maghr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 106, in get_http_url\n",
      "    from_path, content_type = download(link, temp_dir.path)\n",
      "  File \"c:\\Users\\maghr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_internal\\network\\download.py\", line 147, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"c:\\Users\\maghr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_internal\\cli\\progress_bars.py\", line 53, in _rich_progress_bar\n",
      "    for chunk in iterable:\n",
      "  File \"c:\\Users\\maghr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 63, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"c:\\Users\\maghr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 573, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"c:\\Users\\maghr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 509, in read\n",
      "    with self._error_catcher():\n",
      "  File \"c:\\Users\\maghr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\contextlib.py\", line 153, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"c:\\Users\\maghr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 452, in _error_catcher\n",
      "    raise ProtocolError(\"Connection broken: %r\" % e, e)\n",
      "pip._vendor.urllib3.exceptions.ProtocolError: (\"Connection broken: OSError(28, 'No space left on device')\", OSError(28, 'No space left on device'))\n",
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchdiffeq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchdiffeq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m odeint\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
